{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp ocr_idefics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# %reload_ext autoreload\n",
    "# %autoreload 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# install (Colab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try: \n",
    "#     import fastcore as FC\n",
    "# except ImportError: \n",
    "#     !pip install -q fastcore\n",
    "# try:\n",
    "#     import rich\n",
    "# except ImportError:\n",
    "#     !pip install -q rich\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q git+https://github.com/civvic/PanelCleaner.git@basic-tesseract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "need version >4.40 of transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install git+https://github.com/huggingface/transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fash attention doesn't support Metal [#412](https://github.com/Dao-AILab/flash-attention/issues/412) (but see [metal-flash-attention](https://github.com/philipturner/metal-flash-attention))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %env FLASH_ATTENTION_SKIP_CUDA_BUILD=TRUE\n",
    "# %pip install flash-attn --no-build-isolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri May 10 18:32:10 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3090 Ti     On  | 00000000:65:00.0 Off |                  Off |\n",
      "|  0%   50C    P8              33W / 480W |      1MiB / 24564MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing `Idefics` OCR for Comics\n",
    "> Accuracy Enhancements for OCR in `PanelCleaner`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prologue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from __future__ import annotations\n",
    "\n",
    "import functools\n",
    "from pathlib import Path\n",
    "\n",
    "import pcleaner.ocr.ocr as ocr\n",
    "import torch\n",
    "import transformers\n",
    "from pcleaner.ocr.ocr_tesseract import TesseractOcr\n",
    "from PIL import Image\n",
    "from rich.console import Console\n",
    "from transformers import AutoProcessor\n",
    "from transformers import Idefics2ForConditionalGeneration\n",
    "from transformers import PreTrainedModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from typing import cast\n",
    "\n",
    "import fastcore.xtras  # patch Path with some utils\n",
    "import pcleaner.config as cfg\n",
    "from fastcore.test import *  # type: ignore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.41.0.dev0'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformers.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": ""
    }
   },
   "outputs": [],
   "source": [
    "# pretty print by default\n",
    "# %load_ext rich"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": ""
    }
   },
   "outputs": [],
   "source": [
    "#| exporti\n",
    "\n",
    "console = Console(width=104, tab_size=4, force_jupyter=True)\n",
    "cprint = console.print\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Force reload of `experiments` module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'experiments' in sys.modules:\n",
    "    import importlib; importlib.reload(experiments)  # type: ignore\n",
    "else:\n",
    "    import experiments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "from experiments import *\n",
    "from helpers import *\n",
    "from ocr_metric import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "\n",
    "def load_image(img_or_path) -> Image.Image:\n",
    "    if isinstance(img_or_path, (str, Path)):\n",
    "        return Image.open(img_or_path)\n",
    "    elif isinstance(img_or_path, Image.Image):\n",
    "        return img_or_path\n",
    "    else:\n",
    "        raise ValueError(f\"img_or_path must be a path or PIL.Image, got: {type(img_or_path)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# Idefics basic usage\n",
    "\n",
    "not working, cuda memory error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Note that passing the image urls (instead of the actual pil images) to the processor is also possible\n",
    "# # image1 = load_image(\"https://cdn.britannica.com/61/93061-050-99147DCE/Statue-of-Liberty-Island-New-York-Bay.jpg\")\n",
    "# # image2 = load_image(\"https://cdn.britannica.com/59/94459-050-DBA42467/Skyline-Chicago.jpg\")\n",
    "# # image3 = load_image(\"https://cdn.britannica.com/68/170868-050-8DDE8263/Golden-Gate-Bridge-San-Francisco.jpg\")\n",
    "\n",
    "# image1 = Image.open(\"media/Statue-of-Liberty-Island-New-York-Bay.webp\")\n",
    "# image2 = Image.open(\"media/Skyline-Chicago.webp\")\n",
    "# image3 = Image.open(\"media/Golden-Gate-Bridge-San-Francisco.webp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# processor = AutoProcessor.from_pretrained(\"HuggingFaceM4/idefics2-8b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Idefics2ForConditionalGeneration.from_pretrained(\n",
    "#         \"HuggingFaceM4/idefics2-8b\",\n",
    "#         torch_dtype=torch.bfloat16,\n",
    "#         #_attn_implementation=\"flash_attention_2\",\n",
    "#         )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assert isinstance(model, PreTrainedModel)\n",
    "# model.to(DEVICE)\n",
    "# type(model), model.device\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# messages = [\n",
    "#     {\n",
    "#         \"role\": \"user\",\n",
    "#         \"content\": [\n",
    "#             {\"type\": \"image\"},\n",
    "#             {\"type\": \"text\", \"text\": \"What do we see in this image?\"},\n",
    "#         ]\n",
    "#     },\n",
    "#     {\n",
    "#         \"role\": \"assistant\",\n",
    "#         \"content\": [\n",
    "#             {\"type\": \"text\", \"text\": \"In this image, we can see the city of New York, and more specifically the Statue of Liberty.\"},\n",
    "#         ]\n",
    "#     },\n",
    "#     {\n",
    "#         \"role\": \"user\",\n",
    "#         \"content\": [\n",
    "#             {\"type\": \"image\"},\n",
    "#             {\"type\": \"text\", \"text\": \"And how about this image?\"},\n",
    "#         ]\n",
    "#     },       \n",
    "# ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt = processor.apply_chat_template(messages, add_generation_prompt=True)\n",
    "# inputs = processor(text=prompt, images=[image1, image2], return_tensors=\"pt\")\n",
    "# inputs = {k: v.to(DEVICE) for k, v in inputs.items()}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generated_ids = model.generate(**inputs, max_new_tokens=500)\n",
    "# generated_texts = processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "\n",
    "# print(generated_texts)\n",
    "# # ['User: What do we see in this image? \\nAssistant: In this image, we can see the city of New York, and more specifically the Statue of Liberty. \\nUser: And how about this image? \\nAssistant: In this image we can see buildings, trees, lights, water and sky.']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [\n",
    "#     'User: What do we see in this image? '\n",
    "#     'Assistant: In this image, we can see the city of New York, and more specifically the Statue of Liberty. '\n",
    "#     'User: And how about this image? '\n",
    "#     'Assistant: In this image we can see buildings, trees, lights, water and sky.'\n",
    "#     ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# Idefics experiments\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Idefics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Idefics initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "#| exporti\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"HuggingFaceM4/idefics2-8b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a21b61268674a159f210694841c2149",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#| exporti\n",
    "\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model = Idefics2ForConditionalGeneration.from_pretrained(\n",
    "        \"HuggingFaceM4/idefics2-8b\",\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        _attn_implementation=\"flash_attention_2\",\n",
    "        ).to(device)  # type: ignore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(transformers.models.idefics2.modeling_idefics2.Idefics2ForConditionalGeneration,\n",
       " device(type='cuda', index=0))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model), model.device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "\n",
    "prompt_text_tmpl = (\n",
    "        \"Please perform optical character recognition (OCR) on this image, which displays \"\n",
    "        \"speech balloons from a comic book. The text is in {}. Extract the text and \"\n",
    "        \"format it as follows: transcribe in standard sentence case, avoid using all capital \"\n",
    "        \"letters. Provide the transcribed text clearly and double check the sentence is not all capital letters.\")\n",
    "\n",
    "# prompt_text_tmpl = (\"Please perform optical character recognition (OCR) on this image, which displays \"\n",
    "#         f\"speech balloons from a manga comic. The text is in {}. Extract the text and \"\n",
    "#         \"format it without newlines. Provide the transcribed text clearly.\")\n",
    "\n",
    "# prompt_text_tmpl = (\"Please perform optical character recognition (OCR) on this image, which displays \"\n",
    "#         \"speech balloons from a comic book. The text is in {}. Extract the text and \"\n",
    "#         \"format it as follows: transcribe in standard sentence case (avoid using all capital \"\n",
    "#         \"letters) and use asterisks to denote any words that appear in bold within the image. \"\n",
    "#         \"Provide the transcribed text clearly.\")\n",
    "\n",
    "# prompt_text_tmpl = (\"Please perform optical character recognition (OCR) on this image, which displays \"\n",
    "#         \"speech balloons from a comic book. The text is in {}. Extract the text and \"\n",
    "#         \"format it as follows: transcribe in standard sentence case, capitalized. Avoid using \"\n",
    "#         \"all capital letters. In comics, it is common to use two hyphens '--' to interrupt a sentence. \"\n",
    "#         \"Retain any hyphens as they appear in the original text. Provide the transcribed text \"\n",
    "#         \"clearly, ensuring it is capitalized where appropriate, including proper nouns.\")\n",
    "\n",
    "prompt_text_tmpl = (\n",
    "        \"Please perform optical character recognition (OCR) on this image, which displays \"\n",
    "        \"speech balloons from a comic book. The text is in {}. Extract the text and \"\n",
    "        \"format it as follows: transcribe in standard sentence case, capitalized. Avoid using \"\n",
    "        \"all capital letters, but ensure it is capitalized where appropriate, including proper nouns. \"\n",
    "        \"Provide the transcribed text clearly. Double check the text is not all capital letters.\")\n",
    "\n",
    "default_prompt_text_tmpl = prompt_text_tmpl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IdeficsOCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class IdeficsOCR:\n",
    "    prompt_text_tmpl: str = default_prompt_text_tmpl\n",
    "\n",
    "    def __init__(self, \n",
    "            lang: str | None = None, \n",
    "            prompt_text_tmpl: str|None = None, \n",
    "            device: str | None = None\n",
    "        ):\n",
    "        self.lang = lang\n",
    "        self.prompt_text_tmpl = prompt_text_tmpl or self.prompt_text_tmpl\n",
    "        self.device = (device or \n",
    "            \"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    @staticmethod\n",
    "    def is_idefics_available() -> bool:\n",
    "        return True\n",
    "\n",
    "    def _generation_args(self, image: Image.Image, resulting_messages: list[dict]):\n",
    "        prompt = processor.apply_chat_template(resulting_messages, add_generation_prompt=True)\n",
    "        inputs = processor(text=prompt, images=[image], return_tensors=\"pt\")\n",
    "        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "        \n",
    "        max_new_tokens = 512\n",
    "        repetition_penalty = 1.2\n",
    "        decoding_strategy = \"Greedy\"\n",
    "        temperature = 0.4\n",
    "        top_p = 0.8\n",
    "\n",
    "        generation_args = {\n",
    "            \"max_new_tokens\": max_new_tokens,\n",
    "            \"repetition_penalty\": repetition_penalty,\n",
    "        }\n",
    "\n",
    "        assert decoding_strategy in [\n",
    "            \"Greedy\",\n",
    "            \"Top P Sampling\",\n",
    "        ]\n",
    "\n",
    "        if decoding_strategy == \"Greedy\":\n",
    "            generation_args[\"do_sample\"] = False\n",
    "        elif decoding_strategy == \"Top P Sampling\":\n",
    "            generation_args[\"temperature\"] = temperature\n",
    "            generation_args[\"do_sample\"] = True\n",
    "            generation_args[\"top_p\"] = top_p\n",
    "\n",
    "        generation_args.update(inputs)\n",
    "        return prompt, generation_args\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        img_or_path: Image.Image | Path | str,\n",
    "        prompt_text: str | None = None,\n",
    "        lang: str | None = None,\n",
    "        config: str | None = None,\n",
    "        show_prompt: bool = False,\n",
    "        **kwargs,\n",
    "    ) -> str:\n",
    "        if not self.is_idefics_available():\n",
    "            raise RuntimeError(\"Idefics is not installed or not found.\")\n",
    "        resulting_messages = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [{\"type\": \"image\"}] + [\n",
    "                    {\"type\": \"text\", \"text\": prompt_text or self.prompt_text_tmpl.format(lang or self.lang)}\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "        image = load_image(img_or_path)\n",
    "        prompt, generation_args = self._generation_args(image, resulting_messages)\n",
    "        generated_ids = model.generate(**generation_args)\n",
    "        generated_texts = processor.batch_decode(\n",
    "            generated_ids[:, generation_args[\"input_ids\"].size(1):], skip_special_tokens=True)\n",
    "        if show_prompt:\n",
    "            cprint(\"INPUT:\", prompt, \"|OUTPUT:\", generated_texts)\n",
    "        return generated_texts[0]#.strip('\"')\n",
    "\n",
    "    def postprocess_ocr(self, text):\n",
    "        return ' '.join(remove_multiple_whitespaces(text).splitlines())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IdeficsExperimentContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class IdeficsExperimentContext(OCRExperimentContext):\n",
    "    @functools.lru_cache()\n",
    "    def mocr(self, ocr_model: str, lang: str):\n",
    "        if ocr_model == 'Idefics':\n",
    "            proc = IdeficsOCR(lang)\n",
    "        else:\n",
    "            engine = self.engines[ocr_model]\n",
    "            ocr_processor = ocr.get_ocr_processor(True, engine)\n",
    "            proc = ocr_processor[lang2pcleaner(lang)]\n",
    "            if isinstance(proc, TesseractOcr):\n",
    "                proc.lang = lang2tesseract(lang)\n",
    "        return proc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PanelCleaner Configuration\n",
    "> Adapt `PanelCleaner` `Config` current config to this notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = cfg.load_config()\n",
    "config.cache_dir = Path(\".\")\n",
    "\n",
    "cache_dir = config.get_cleaner_cache_dir()\n",
    "\n",
    "profile = config.current_profile\n",
    "preprocessor_conf = profile.preprocessor\n",
    "# Modify the profile to OCR all boxes.\n",
    "# Make sure OCR is enabled.\n",
    "preprocessor_conf.ocr_enabled = True\n",
    "# Make sure the max size is infinite, so no boxes are skipped in the OCR process.\n",
    "preprocessor_conf.ocr_max_size = 10**10\n",
    "# Make sure the sus box min size is infinite, so all boxes with \"unknown\" language are skipped.\n",
    "preprocessor_conf.suspicious_box_min_size = 10**10\n",
    "# Set the OCR blacklist pattern to match everything, so all text gets reported in the analytics.\n",
    "preprocessor_conf.ocr_blacklist_pattern = \".*\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test images\n",
    "> `IMAGE_PATHS` is a list of image file paths that are used as input for testing the OCR methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['00: Action_Comics_1960-01-00_(262).JPG',\n",
       " '01: Adolf_Cap_01_008.jpg',\n",
       " '02: Barnaby_v1-028.png',\n",
       " '03: Barnaby_v1-029.png',\n",
       " '04: Buck_Danny_-_12_-_Avions_Sans_Pilotes_-_013.jpg',\n",
       " '05: Cannon-292.jpg',\n",
       " '06: Contrato_con_Dios_028.jpg',\n",
       " '07: Erase_una_vez_en_Francia_02_88.jpg',\n",
       " '08: FOX_CHILLINTALES_T17_012.jpg',\n",
       " '09: Furari_-_Jiro_Taniguchi_selma_056.jpg',\n",
       " '10: Galactus_12.jpg',\n",
       " '11: INOUE_KYOUMEN_002.png',\n",
       " '12: MCCALL_ROBINHOOD_T31_010.jpg',\n",
       " '13: MCCAY_LITTLENEMO_090.jpg',\n",
       " '14: Mary_Perkins_On_Stage_v2006_1_-_P00068.jpg',\n",
       " '15: PIKE_BOYLOVEGIRLS_T41_012.jpg',\n",
       " '16: Sal_Buscema_Spaceknights_&_Superheroes_Ocular_Edition_1_1.png',\n",
       " '17: Sal_Buscema_Spaceknights_&_Superheroes_Ocular_Edition_1_1_K.png',\n",
       " '18: Sal_Buscema_Spaceknights_&_Superheroes_Ocular_Edition_1_2.png',\n",
       " '19: Spirou_Et_Fantasio_Integrale_06_1958_1959_0025_0024.jpg',\n",
       " '20: Strange_Tales_172005.jpg',\n",
       " '21: Strange_Tales_172021.jpg',\n",
       " '22: Tarzan_014-21.JPG',\n",
       " '23: Tintin_21_Les_Bijoux_de_la_Castafiore_page_39.jpg',\n",
       " '24: Transformers_-_Unicron_000-004.jpg',\n",
       " '25: Transformers_-_Unicron_000-016.jpg',\n",
       " '26: WARE_ACME_024.jpg',\n",
       " '27: Yoko_Tsuno_T01_1972-10.jpg',\n",
       " '28: Your_Name_Another_Side_Earthbound_T02_084.jpg',\n",
       " '29: manga_0033.jpg',\n",
       " '30: ronson-031.jpg',\n",
       " '31: 哀心迷図のバベル 第01巻 - 22002_00_059.jpg']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "media_path = Path(\"media/\")\n",
    "\n",
    "IMAGE_PATHS = sorted(\n",
    "    [_ for _ in media_path.glob(\"*\") if _.is_file() and _.suffix.lower() in [\".jpg\", \".png\", \".jpeg\"]])\n",
    "\n",
    "[f\"{i:02}: {_.name}\" for i,_ in enumerate(IMAGE_PATHS)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CONTEXT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Configuration:\n",
      "\n",
      "Locale: System default\n",
      "Default Profile: Built-in\n",
      "Saved Profiles:\n",
      "- victess: /home/vic/dev/repo/DL-mac/cleaned/victess.conf\n",
      "- victmang: /home/vic/dev/repo/DL-mac/cleaned/vicmang.conf\n",
      "\n",
      "Profile Editor: System default\n",
      "Cache Directory: .\n",
      "Default Torch Model Path: /home/vic/.cache/pcleaner/model/comictextdetector.pt\n",
      "Default CV2 Model Path: /home/vic/.cache/pcleaner/model/comictextdetector.pt.onnx\n",
      "GUI Theme: System default\n",
      "\n",
      "--------------------\n",
      "\n",
      "Config file located at: /home/vic/.config/pcleaner/pcleanerrc\n",
      "System default cache directory: /home/vic/.cache/pcleaner\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">      cache_dir: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Path</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'cleaner'</span><span style=\"font-weight: bold\">)</span>\n",
       "     model_path: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Path</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'/home/vic/.cache/pcleaner/model/comictextdetector.pt'</span><span style=\"font-weight: bold\">)</span>\n",
       "         device: <span style=\"color: #008000; text-decoration-color: #008000\">'cuda'</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "      cache_dir: \u001b[1;35mPath\u001b[0m\u001b[1m(\u001b[0m\u001b[32m'cleaner'\u001b[0m\u001b[1m)\u001b[0m\n",
       "     model_path: \u001b[1;35mPath\u001b[0m\u001b[1m(\u001b[0m\u001b[32m'/home/vic/.cache/pcleaner/model/comictextdetector.pt'\u001b[0m\u001b[1m)\u001b[0m\n",
       "         device: \u001b[32m'cuda'\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "CONTEXT = IdeficsExperimentContext(None, IMAGE_PATHS)\n",
    "\n",
    "gpu = torch.cuda.is_available() or torch.backends.mps.is_available()\n",
    "model_path = CONTEXT.config.get_model_path(gpu)\n",
    "DEVICE = (\"mps\" if torch.backends.mps.is_available() else \"cuda\") if model_path.suffix == \".pt\" else \"cpu\"\n",
    "\n",
    "CONTEXT.config.show()\n",
    "cprint(\n",
    "    f\"{'cache_dir':>15}: {repr(cache_dir)}\\n\"\n",
    "    f\"{'model_path':>15}: {repr(model_path)}\\n\"\n",
    "    f\"{'device':>15}: {repr(DEVICE)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base image\n",
    "> Change `BASE_IMAGE_IDX` to select a different base image to use in the examples below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_IMAGE_IDX: ImgIdT = cast(ImgIdT, CONTEXT.normalize_idx(\"Strange_Tales_172005.jpg\"))\n",
    "assert CONTEXT.path_from_idx(BASE_IMAGE_IDX).exists()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_CONTEXT = ImageContext(CONTEXT, BASE_IMAGE_IDX)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c0379d4776a4e0f9facd7e0092c79f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output(layout=Layout(height='0px'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f0ae88943a14ea38efb029c40183ffd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HBox(children=(Dropdown(index=20, layout=Layout(width='fit-content'), options={'Action_Comics_1…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26c6ecdbae6f4952aab7ff3106400f04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "img_visor = ImageContextVisor(CONTEXT, BASE_IMAGE_IDX)\n",
    "img_visor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Box id\n",
    "> change `BOX_IDX` to use any box to test crop methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOX_IDX = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Idefics inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "page_lang = IMAGE_CONTEXT.page_lang\n",
    "\n",
    "resulting_messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [{\"type\": \"image\"}] + [\n",
    "            {\"type\": \"text\", \"text\": prompt_text_tmpl.format(page_lang)}\n",
    "        ]\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def idefics_generation_args(image: Image.Image, resulting_messages: list[dict]):\n",
    "    prompt = processor.apply_chat_template(resulting_messages, add_generation_prompt=True)\n",
    "    inputs = processor(text=prompt, images=[image], return_tensors=\"pt\")\n",
    "    inputs = {k: v.to(DEVICE) for k, v in inputs.items()}\n",
    "    \n",
    "    max_new_tokens = 512\n",
    "    repetition_penalty = 1.2\n",
    "    decoding_strategy = \"Greedy\"\n",
    "    temperature = 0.4\n",
    "    top_p = 0.8\n",
    "\n",
    "    generation_args = {\n",
    "        \"max_new_tokens\": max_new_tokens,\n",
    "        \"repetition_penalty\": repetition_penalty,\n",
    "    }\n",
    "\n",
    "    assert decoding_strategy in [\n",
    "        \"Greedy\",\n",
    "        \"Top P Sampling\",\n",
    "    ]\n",
    "\n",
    "    if decoding_strategy == \"Greedy\":\n",
    "        generation_args[\"do_sample\"] = False\n",
    "    elif decoding_strategy == \"Top P Sampling\":\n",
    "        generation_args[\"temperature\"] = temperature\n",
    "        generation_args[\"do_sample\"] = True\n",
    "        generation_args[\"top_p\"] = top_p\n",
    "\n",
    "    generation_args.update(inputs)\n",
    "    return prompt, generation_args\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crop methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_experiment = ExperimentOCR(IMAGE_CONTEXT, 'Idefics')\n",
    "image_experiment = ExperimentOCR.from_image(CONTEXT, 'Idefics', IMAGE_CONTEXT.image_idx)  # use cache\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "method = CropMethod.INITIAL_BOX\n",
    "\n",
    "result = cast(ResultOCR, image_experiment.result(BOX_IDX, method, ocr=False))\n",
    "image = cast(Image.Image, result.image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No chat template is set for this tokenizer, falling back to a default class-level template. This is very error-prone, because models are often trained with templates different from the class default! Default chat templates are a legacy feature and will be removed in Transformers v4.43, at which point any code depending on them will stop working. We recommend setting a valid chat template before then to ensure that this model continues working without issues.\n",
      "The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">INPUT: User:<span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">image</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;Please perform optical character recognition </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #000000; text-decoration-color: #000000\">OCR</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\"> on this image, which displays </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">speech balloons from a comic book. The text is in English. Extract the text and format it as follows: </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">transcribe in standard sentence case, capitalized. Avoid using all capital letters, but ensure it is </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">capitalized where appropriate, including proper nouns. Provide the transcribed text clearly. Double </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">check the text is not all capital letters.&lt;end_of_utterance</span><span style=\"font-weight: bold\">&gt;</span>\n",
       "Assistant: |OUTPUT:\n",
       "<span style=\"font-weight: bold\">[</span>\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'Embowered by great gnarled cypress trees, the ancient manor stands alone on the outskirts of New </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Orleans, kept tidy by a white-haired old man known only as Bambu.'</span>\n",
       "<span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "INPUT: User:\u001b[1m<\u001b[0m\u001b[1;95mimage\u001b[0m\u001b[39m>Please perform optical character recognition \u001b[0m\u001b[1;39m(\u001b[0m\u001b[39mOCR\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m on this image, which displays \u001b[0m\n",
       "\u001b[39mspeech balloons from a comic book. The text is in English. Extract the text and format it as follows: \u001b[0m\n",
       "\u001b[39mtranscribe in standard sentence case, capitalized. Avoid using all capital letters, but ensure it is \u001b[0m\n",
       "\u001b[39mcapitalized where appropriate, including proper nouns. Provide the transcribed text clearly. Double \u001b[0m\n",
       "\u001b[39mcheck the text is not all capital letters.<end_of_utterance\u001b[0m\u001b[1m>\u001b[0m\n",
       "Assistant: |OUTPUT:\n",
       "\u001b[1m[\u001b[0m\n",
       "    \u001b[32m'Embowered by great gnarled cypress trees, the ancient manor stands alone on the outskirts of New \u001b[0m\n",
       "\u001b[32mOrleans, kept tidy by a white-haired old man known only as Bambu.'\u001b[0m\n",
       "\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt, generation_args = idefics_generation_args(image, resulting_messages)\n",
    "generated_ids = model.generate(**generation_args)\n",
    "\n",
    "generated_texts = processor.batch_decode(\n",
    "    generated_ids[:, generation_args[\"input_ids\"].size(1):], skip_special_tokens=True)\n",
    "cprint(\"INPUT:\", prompt, \"|OUTPUT:\", generated_texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><colgroup><col style='width: 370px;'/><col style='width: auto;'/></colgroup><tr><td style='text-align: center;'><img src=\"cleaner/Strange_Tales_172005/Strange_Tales_172005_0_Initial box.png\"/></td><td style='font-size: 12pt; text-align: left; '>Embowered by great gnarled cypress trees, the ancient manor stands alone on the outskirts of New Orleans, kept tidy by a white-haired old man known only as Bambu.<br/><strong><span style='color: red;'>1.00</span></strong></td></tr></table>\n",
       "<br/>\n",
       "<pre style='font-size: 14px;'><div style='font-family: monospace; white-space: pre-wrap;'>Embowered by great gnarled cypress trees, the ancient manor stands alone on the outskirts of New Orleans, kept tidy by a white-haired old man known only as Bambu.</div><br/><div style='font-family: monospace; white-space: pre-wrap;'>Embowered by great gnarled cypress trees, the ancient manor stands alone on the outskirts of New Orleans, kept tidy by a white-haired old man known only as Bambu.</div></pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "result.ocr = generated_texts[0]\n",
    "result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">INPUT: User:<span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">image</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;Please perform optical character recognition </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #000000; text-decoration-color: #000000\">OCR</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\"> on this image, which displays </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">speech balloons from a comic book. The text is in English. Extract the text and format it as follows: </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">transcribe in standard sentence case, capitalized. Avoid using all capital letters, but ensure it is </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">capitalized where appropriate, including proper nouns. Provide the transcribed text clearly. Double </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">check the text is not all capital letters.&lt;end_of_utterance</span><span style=\"font-weight: bold\">&gt;</span>\n",
       "Assistant: |OUTPUT:\n",
       "<span style=\"font-weight: bold\">[</span>\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'Embowered by great gnarled cypress trees, the ancient manor stands alone on the outskirts of New </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Orleans, kept tidy by a white-haired old man known only as Bambu.'</span>\n",
       "<span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "INPUT: User:\u001b[1m<\u001b[0m\u001b[1;95mimage\u001b[0m\u001b[39m>Please perform optical character recognition \u001b[0m\u001b[1;39m(\u001b[0m\u001b[39mOCR\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m on this image, which displays \u001b[0m\n",
       "\u001b[39mspeech balloons from a comic book. The text is in English. Extract the text and format it as follows: \u001b[0m\n",
       "\u001b[39mtranscribe in standard sentence case, capitalized. Avoid using all capital letters, but ensure it is \u001b[0m\n",
       "\u001b[39mcapitalized where appropriate, including proper nouns. Provide the transcribed text clearly. Double \u001b[0m\n",
       "\u001b[39mcheck the text is not all capital letters.<end_of_utterance\u001b[0m\u001b[1m>\u001b[0m\n",
       "Assistant: |OUTPUT:\n",
       "\u001b[1m[\u001b[0m\n",
       "    \u001b[32m'Embowered by great gnarled cypress trees, the ancient manor stands alone on the outskirts of New \u001b[0m\n",
       "\u001b[32mOrleans, kept tidy by a white-haired old man known only as Bambu.'\u001b[0m\n",
       "\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table><colgroup><col style='width: 370px;'/><col style='width: auto;'/></colgroup><tr><td style='text-align: center;'><img src=\"cleaner/Strange_Tales_172005/Strange_Tales_172005_0_Initial box.png\"/></td><td style='font-size: 12pt; text-align: left; '>Embowered by great gnarled cypress trees, the ancient manor stands alone on the outskirts of New Orleans, kept tidy by a white-haired old man known only as Bambu.<br/><strong><span style='color: red;'>1.00</span></strong></td></tr></table>\n",
       "<br/>\n",
       "<pre style='font-size: 14px;'><div style='font-family: monospace; white-space: pre-wrap;'>Embowered by great gnarled cypress trees, the ancient manor stands alone on the outskirts of New Orleans, kept tidy by a white-haired old man known only as Bambu.</div><br/><div style='font-family: monospace; white-space: pre-wrap;'>Embowered by great gnarled cypress trees, the ancient manor stands alone on the outskirts of New Orleans, kept tidy by a white-haired old man known only as Bambu.</div></pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "method = CropMethod.INITIAL_BOX\n",
    "\n",
    "result = cast(ResultOCR, image_experiment.result(BOX_IDX, method, ocr=False))\n",
    "image = cast(Image.Image, result.image)\n",
    "\n",
    "mocr: IdeficsOCR = cast(IdeficsOCR, CONTEXT.mocr('Idefics', page_lang))\n",
    "text = mocr(image, show_prompt=True)\n",
    "result.ocr = mocr.postprocess_ocr(text)\n",
    "result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><colgroup><col style='width: 378px;'/><col style='width: auto;'/></colgroup><tr><td style='text-align: center;'><img src=\"cleaner/Strange_Tales_172005/Strange_Tales_172005_0_Padded 4px.png\"/></td><td style='font-size: 12pt; text-align: left; '>Embowered by great gnarled cypress trees, the ancient manor stands alone on the outskirts of new orleans, kept tidy by a white-haired old man known only as bambu.<br/><strong><span style='color: red;'>0.98</span></strong></td></tr></table>\n",
       "<br/>\n",
       "<pre style='font-size: 14px;'><div style='font-family: monospace; white-space: pre-wrap;'>Embowered by great gnarled cypress trees, the ancient manor stands alone on the outskirts of New Orleans, kept tidy by a white-haired old man known only as Bambu.</div><br/><div style='font-family: monospace; white-space: pre-wrap;'>Embowered by great gnarled cypress trees, the ancient manor stands alone on the outskirts of <span style='color: red;'>n</span>ew <span style='color: red;'>o</span>rleans, kept tidy by a white-haired old man known only as <span style='color: red;'>b</span>ambu.</div></pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image_experiment.result(BOX_IDX, CropMethod.PADDED_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr><td style='text-align: center;'><img src=\"cleaner/Strange_Tales_172005/Strange_Tales_172005_0_Pad 8, fract. 0.2_cropped.png\"/></td><td style='text-align: center;'><img src=\"cleaner/Strange_Tales_172005/Strange_Tales_172005_0_Pad 8, fract. 0.2_mask.png\"/></td><td style='text-align: center;'><img src=\"cleaner/Strange_Tales_172005/Strange_Tales_172005_0_Pad 8, fract. 0.2.png\"/></td></tr></table>\n",
       "<br/>\n",
       "<div style='font-size: 12pt;'><strong style='color: red;'>0.94</strong><div/>\n",
       "<br/>\n",
       "<pre style='font-size: 14px;'><div style='font-family: monospace; white-space: pre-wrap;'>Embow<span style='color: green;'>&#x2395;&#x2395;</span>ered by great gnarled cypress trees, the ancient manor stands alone on the outskirts of New Orleans, kept tidy by a white-haired old man known only as Bambu.</div><br/><div style='font-family: monospace; white-space: pre-wrap;'>E<span style='color: red;'>nc</span>o<span style='color: red;'>unt</span>ered by great <span style='color: red;'>ch</span>arle<span style='color: red;'>s</span> cypress trees, the ancient manor stands alone on the outskirts of <span style='color: red;'>n</span>ew <span style='color: red;'>o</span>rleans, kept tidy by a white-haired old man known only as <span style='color: red;'>b</span>ambu.</div></pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image_experiment.result(BOX_IDX, CropMethod.PAD_8_FRACT_0_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# Visualize results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "522220949ff540fdbd864dc8d8722cf0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output(layout=Layout(height='0px'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13a3ec1b0ea04630b339e5026e01eee2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HBox(children=(Label(value='Box # (of 15):', layout=Layout(padding='0px 0px 0px 10px', width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "989b4acac70848f2a5da0a74f99bc181",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "result_visor = ResultVisor(image_experiment)\n",
    "result_visor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# Visualize Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p, d = image_experiment.to_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bae922f0f53b47b5909334ca7f5d24fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HBox(children=(Dropdown(index=20, layout=Layout(width='fit-content'), options={'Action_Comics_1…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c8a0db76d0a40eb9c3451129e803124",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "exp_visor = ExperimentVisor(image_experiment)\n",
    "exp_visor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p, d = exp_visor.ctx.to_json()\n",
    "# p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# EEAaO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d441a9415ca94481a85523a6d30eca92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HBox(children=(HBox(children=(Dropdown(index=1, layout=Layout(width='fit-content'), options={'T…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f568a40794a4cba951fb6652b5446dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "idefics_experiment = ExperimentsVisor(CONTEXT, BASE_IMAGE_IDX, \n",
    "                            box_idx=13, method=CropMethod.DEFAULT_GREY_PAD,\n",
    "                            ocr_model=OCRModel.IDEFICS, \n",
    "                            ocr_models={'Tesseract': OCRModel.TESSERACT, 'Idefics': OCRModel.IDEFICS})\n",
    "idefics_experiment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "idefics_experiment.update(model=OCRModel.TESSERACT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Colophon\n",
    "----\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fastcore.all as FC\n",
    "from nbdev.export import nb_export\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "if FC.IN_NOTEBOOK:\n",
    "    nb_export('test_idefics.ipynb', '.')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "panel-cleaner",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
