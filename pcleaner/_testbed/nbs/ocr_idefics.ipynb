{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp testbed/ocr_idefics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "from __future__ import annotations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# %reload_ext autoreload\n",
    "# %autoreload 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing `Idefics` OCR for Comics\n",
    "> Accuracy Enhancements for OCR in `PanelCleaner`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prologue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import functools\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "from typing import Any\n",
    "from typing import Literal\n",
    "from typing import TypeAlias\n",
    "\n",
    "import pcleaner.config as cfg\n",
    "import pcleaner.ocr.ocr as ocr\n",
    "import torch\n",
    "from pcleaner.ocr.ocr_tesseract import TesseractOcr\n",
    "from PIL import Image\n",
    "from rich.console import Console\n",
    "from transformers import AutoProcessor\n",
    "from transformers import Idefics2ForConditionalGeneration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "from typing import cast\n",
    "\n",
    "import fastcore.all as FC\n",
    "import fastcore.xtras  # patch pathlib.Path with some utils\n",
    "import transformers\n",
    "from fastcore.test import *  # type: ignore\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "need version >4.40 of transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.40.2'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformers.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install git+https://github.com/huggingface/transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fash attention doesn't support Metal [#412](https://github.com/Dao-AILab/flash-attention/issues/412) (but see [metal-flash-attention](https://github.com/philipturner/metal-flash-attention))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %env FLASH_ATTENTION_SKIP_CUDA_BUILD=TRUE\n",
    "# %pip install flash-attn --no-build-isolation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pretty print by default\n",
    "# %load_ext rich"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "\n",
    "console = Console(width=104, tab_size=4, force_jupyter=True)\n",
    "cprint = console.print\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Force reload of `experiments` module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'pcleaner._testbed.testbed.experiments' in sys.modules:\n",
    "    import importlib; importlib.reload(pcleaner._testbed.testbed.experiments)  # type: ignore\n",
    "else:\n",
    "    import pcleaner._testbed.testbed.experiments\n",
    "    from pcleaner._testbed.testbed.experiments import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "\n",
    "import pcleaner._testbed.testbed.experiments as exp_testbed\n",
    "from pcleaner._testbed.testbed.experiments import *\n",
    "from pcleaner._testbed.testbed.helpers import RenderJSON\n",
    "import pcleaner._testbed.testbed.web_server as web_server\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "\n",
    "def load_image(img_or_path) -> Image.Image:\n",
    "    if isinstance(img_or_path, (str, Path)):\n",
    "        return Image.open(img_or_path)\n",
    "    elif isinstance(img_or_path, Image.Image):\n",
    "        return img_or_path\n",
    "    else:\n",
    "        raise ValueError(f\"img_or_path must be a path or PIL.Image, got: {type(img_or_path)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "\n",
    "def get_gpu_vram(total=True):\n",
    "    if total:\n",
    "        command = \"nvidia-smi --query-gpu=memory.total --format=csv,noheader,nounits\"\n",
    "    else:\n",
    "        command = \"nvidia-smi --query-gpu=memory.used --format=csv,noheader,nounits\"\n",
    "    try:\n",
    "        vram = subprocess.check_output(command, shell=True).decode('utf-8').strip()\n",
    "        return vram\n",
    "    except subprocess.CalledProcessError:\n",
    "        return \"Failed to get VRAM\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed May 22 11:58:01 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3090 Ti     On  | 00000000:65:00.0 Off |                  Off |\n",
      "|  0%   46C    P8              23W / 480W |      3MiB / 24564MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Nvidia card total VRAM: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">24564</span>  MiB\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Nvidia card total VRAM: \u001b[1;36m24564\u001b[0m  MiB\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Nvidia card current VRAM: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>  MiB\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Nvidia card current VRAM: \u001b[1;36m3\u001b[0m  MiB\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cprint(f\"Nvidia card total VRAM: {get_gpu_vram()}  MiB\")\n",
    "cprint(f\"Nvidia card current VRAM: {get_gpu_vram(False)}  MiB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# Idefics basic usage\n",
    "\n",
    "not working, cuda memory error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Note that passing the image urls (instead of the actual pil images) to the processor is also possible\n",
    "# # image1 = load_image(\"https://cdn.britannica.com/61/93061-050-99147DCE/Statue-of-Liberty-Island-New-York-Bay.jpg\")\n",
    "# # image2 = load_image(\"https://cdn.britannica.com/59/94459-050-DBA42467/Skyline-Chicago.jpg\")\n",
    "# # image3 = load_image(\"https://cdn.britannica.com/68/170868-050-8DDE8263/Golden-Gate-Bridge-San-Francisco.jpg\")\n",
    "\n",
    "# image1 = Image.open(\"media/Statue-of-Liberty-Island-New-York-Bay.webp\")\n",
    "# image2 = Image.open(\"media/Skyline-Chicago.webp\")\n",
    "# image3 = Image.open(\"media/Golden-Gate-Bridge-San-Francisco.webp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# processor = AutoProcessor.from_pretrained(\"HuggingFaceM4/idefics2-8b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Idefics2ForConditionalGeneration.from_pretrained(\n",
    "#         \"HuggingFaceM4/idefics2-8b\",\n",
    "#         torch_dtype=torch.bfloat16,\n",
    "#         #_attn_implementation=\"flash_attention_2\",\n",
    "#         )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assert isinstance(model, PreTrainedModel)\n",
    "# model.to(DEVICE)\n",
    "# type(model), model.device\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# messages = [\n",
    "#     {\n",
    "#         \"role\": \"user\",\n",
    "#         \"content\": [\n",
    "#             {\"type\": \"image\"},\n",
    "#             {\"type\": \"text\", \"text\": \"What do we see in this image?\"},\n",
    "#         ]\n",
    "#     },\n",
    "#     {\n",
    "#         \"role\": \"assistant\",\n",
    "#         \"content\": [\n",
    "#             {\"type\": \"text\", \"text\": \"In this image, we can see the city of New York, and more specifically the Statue of Liberty.\"},\n",
    "#         ]\n",
    "#     },\n",
    "#     {\n",
    "#         \"role\": \"user\",\n",
    "#         \"content\": [\n",
    "#             {\"type\": \"image\"},\n",
    "#             {\"type\": \"text\", \"text\": \"And how about this image?\"},\n",
    "#         ]\n",
    "#     },       \n",
    "# ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt = processor.apply_chat_template(messages, add_generation_prompt=True)\n",
    "# inputs = processor(text=prompt, images=[image1, image2], return_tensors=\"pt\")\n",
    "# inputs = {k: v.to(DEVICE) for k, v in inputs.items()}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generated_ids = model.generate(**inputs, max_new_tokens=500)\n",
    "# generated_texts = processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "\n",
    "# print(generated_texts)\n",
    "# # ['User: What do we see in this image? \\nAssistant: In this image, we can see the city of New York, and more specifically the Statue of Liberty. \\nUser: And how about this image? \\nAssistant: In this image we can see buildings, trees, lights, water and sky.']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [\n",
    "#     'User: What do we see in this image? '\n",
    "#     'Assistant: In this image, we can see the city of New York, and more specifically the Statue of Liberty. '\n",
    "#     'User: And how about this image? '\n",
    "#     'Assistant: In this image we can see buildings, trees, lights, water and sky.'\n",
    "#     ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# Idefics experiments\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXP_DIR = \"../experiment\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup ngrok (Colab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The experiments can generate hundreds of images, and maintaining the **PIL** images in memory is not efficient. All the generated images are cached and visualized on demand through a URL pointing to the local cache. This approach prevents the kernel from being overloaded with **PIL** images, with the front-end responsible for fetching the image and the backend web server (not the kernel) for serving the image in another process. This method is quick and efficient. As an added bonus, the saved notebook remains lean and fit; it doesn't store the Base64 versions of all the output cell images.\n",
    "\n",
    "Unfortunately, this approach does not work as is in **Colab**. Google Colab runs on an older Ubuntu 18.04 VM, so all the usual networking challenges with Docker, or whatever VMs Google is using, apply. Google also goes to great lengths to avoid exposing its internal architecture. We have two options:\n",
    "- Let the Jupyter kernel serve the images itself, which is slow and memory-consuming.\n",
    "- Use a tunnel to map localhost (server) to whatever IP and port the front-end (the browser you're currently using) is running on. We can use **ngrok** for this, but *ngrok* is a commercial service that has been abused and now requires confirmation the first time the tunnel connects, which can be inconvenient for the user. It also requires the user to open a free account and obtain an auth token.\n",
    "\n",
    "You choose.\n",
    "\n",
    "If the notebook is running in Colab and ngrok has been successfully installed and the tunnel has been created, the default setting is USE_PIL=False. You can set the environment variable USE_PIL=True to force the use of PIL images, but note that in certain circumstances, Colab will complain because the free tiers are usually memory constrained.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['USE_PIL'] = 'False'\n",
    "os.environ['USE_TUNNEL'] = 'False'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SERVER = None\n",
    "if (os.environ['USE_PIL'].lower() == 'false') and os.environ['USE_TUNNEL'].lower() == 'true':\n",
    "    SERVER = web_server.setup_ngrok(web_server.WebServerBottle, Path(EXP_DIR))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Idefics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Idefics initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "\n",
    "def _setup_processor():\n",
    "    return AutoProcessor.from_pretrained(\n",
    "        \"HuggingFaceM4/idefics2-8b\", \n",
    "        do_image_splitting=False  #  cropped boxes are usually small\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "\n",
    "QuantT: TypeAlias = Literal['bfloat16'] | Literal['8bits'] | Literal['4bits']\n",
    "\n",
    "def _setup_model(quant: QuantT, flashattn: bool=True):\n",
    "    kwargs: dict = dict(\n",
    "        torch_dtype=torch.bfloat16,\n",
    "    )\n",
    "    if quant == 'bfloat16':\n",
    "        pass\n",
    "    else:\n",
    "        from transformers import BitsAndBytesConfig\n",
    "        quantization_config = None\n",
    "        if quant == '8bits':\n",
    "            quantization_config = BitsAndBytesConfig(\n",
    "                load_in_8bit=True,\n",
    "            )\n",
    "        if quant == '4bits':\n",
    "            quantization_config = BitsAndBytesConfig(\n",
    "                load_in_4bit=True,\n",
    "                bnb_4bit_quant_type=\"nf4\",\n",
    "                bnb_4bit_use_double_quant=True,\n",
    "                bnb_4bit_compute_dtype=torch.float16\n",
    "            )\n",
    "        if quantization_config is not None:\n",
    "            kwargs.update(quantization_config=quantization_config)\n",
    "    if flashattn:\n",
    "        kwargs.update(_attn_implementation=\"flash_attention_2\")\n",
    "    model = Idefics2ForConditionalGeneration.from_pretrained(\n",
    "        \"HuggingFaceM4/idefics2-8b\", \n",
    "        device_map='auto', \n",
    "        **kwargs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "\n",
    "prompt_text_tmpl = (\n",
    "        \"Please perform optical character recognition (OCR) on this image, which displays \"\n",
    "        \"speech balloons from a comic book. The text is in {}. Extract the text and \"\n",
    "        \"format it as follows: transcribe in standard sentence case, avoid using all capital \"\n",
    "        \"letters. Provide the transcribed text clearly and double check the sentence is not all capital letters.\")\n",
    "\n",
    "# prompt_text_tmpl = (\"Please perform optical character recognition (OCR) on this image, which displays \"\n",
    "#         f\"speech balloons from a manga comic. The text is in {}. Extract the text and \"\n",
    "#         \"format it without newlines. Provide the transcribed text clearly.\")\n",
    "\n",
    "# prompt_text_tmpl = (\"Please perform optical character recognition (OCR) on this image, which displays \"\n",
    "#         \"speech balloons from a comic book. The text is in {}. Extract the text and \"\n",
    "#         \"format it as follows: transcribe in standard sentence case (avoid using all capital \"\n",
    "#         \"letters) and use asterisks to denote any words that appear in bold within the image. \"\n",
    "#         \"Provide the transcribed text clearly.\")\n",
    "\n",
    "# prompt_text_tmpl = (\"Please perform optical character recognition (OCR) on this image, which displays \"\n",
    "#         \"speech balloons from a comic book. The text is in {}. Extract the text and \"\n",
    "#         \"format it as follows: transcribe in standard sentence case, capitalized. Avoid using \"\n",
    "#         \"all capital letters. In comics, it is common to use two hyphens '--' to interrupt a sentence. \"\n",
    "#         \"Retain any hyphens as they appear in the original text. Provide the transcribed text \"\n",
    "#         \"clearly, ensuring it is capitalized where appropriate, including proper nouns.\")\n",
    "\n",
    "prompt_text_tmpl = (\n",
    "        \"Please perform optical character recognition (OCR) on this image, which displays \"\n",
    "        \"speech balloons from a comic book. The text is in {}. Extract the text and \"\n",
    "        \"format it as follows: transcribe in standard sentence case, capitalized. Avoid using \"\n",
    "        \"all capital letters, but ensure it is capitalized where appropriate, including proper nouns. \"\n",
    "        \"Provide the transcribed text clearly. Double check the text is not all capital letters.\")\n",
    "\n",
    "\n",
    "# prompt_text_tmpl = (\n",
    "#         \"Please perform optical character recognition (OCR) on this image, which contains speech \"\n",
    "#         \"balloons from a comic book. The text is in English. Carefully transcribe the text, \"\n",
    "#         \"ensuring that you preserve the original formatting and line breaks as they appear \"\n",
    "#         \"in the speech balloon.\"\n",
    "# )\n",
    "\n",
    "default_prompt_text_tmpl = prompt_text_tmpl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IdeficsOCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class IdeficsOCR:\n",
    "    prompt_text_tmpl: str = default_prompt_text_tmpl\n",
    "    PROCESSOR: Any = None\n",
    "    MODEL: Any = None\n",
    "\n",
    "\n",
    "    @classmethod\n",
    "    def setup_processor(cls):\n",
    "        cls.PROCESSOR = _setup_processor()\n",
    "        return cls.PROCESSOR\n",
    "    \n",
    "    @classmethod\n",
    "    def setup_model(cls, quant: QuantT='bfloat16', flashattn: bool=True):\n",
    "        cls.MODEL = _setup_model(quant, flashattn)\n",
    "        return cls.MODEL\n",
    "    \n",
    "    @staticmethod\n",
    "    def is_idefics_available() -> bool:\n",
    "        return True\n",
    "\n",
    "    def show_info(self):\n",
    "        cprint(\n",
    "            f\"{'model':>17}: {type(self.MODEL)}\\n\"\n",
    "            f\"{'quantization':>17}: {type(self.quant)}\\n\"\n",
    "            f\"{'device':>17}: {repr(self.MODEL.device)}\\n\"\n",
    "            f\"{'current VRAM':>17}: {get_gpu_vram(False)}  MiB\\n\"\n",
    "    )\n",
    "\n",
    "\n",
    "    def __init__(self, \n",
    "            lang: str | None = None, \n",
    "            prompt_text_tmpl: str|None = None, \n",
    "            quant: QuantT | None = None,\n",
    "            flashattn: bool | None = None,\n",
    "        ):\n",
    "        self.lang = lang\n",
    "        self.prompt_text_tmpl = prompt_text_tmpl or self.prompt_text_tmpl\n",
    "        self.quant = quant or 'bfloat16'#'4bits'\n",
    "        self.flashattn = flashattn or True\n",
    "        if self.PROCESSOR is None:\n",
    "            type(self).setup_processor()\n",
    "        if self.MODEL is None:\n",
    "            type(self).setup_model(self.quant, self.flashattn)\n",
    "        self.device = self.MODEL.device\n",
    "\n",
    "    def _generation_args(self, image: Image.Image, resulting_messages: list[dict]):\n",
    "        prompt = self.PROCESSOR.apply_chat_template(resulting_messages, add_generation_prompt=True)\n",
    "        inputs = self.PROCESSOR(text=prompt, images=[image], return_tensors=\"pt\")\n",
    "        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "        \n",
    "        max_new_tokens = 512\n",
    "        repetition_penalty = 1.2\n",
    "        decoding_strategy = \"Greedy\"\n",
    "        temperature = 0.4\n",
    "        top_p = 0.8\n",
    "\n",
    "        generation_args = {\n",
    "            \"max_new_tokens\": max_new_tokens,\n",
    "            \"repetition_penalty\": repetition_penalty,\n",
    "        }\n",
    "\n",
    "        assert decoding_strategy in [\n",
    "            \"Greedy\",\n",
    "            \"Top P Sampling\",\n",
    "        ]\n",
    "\n",
    "        if decoding_strategy == \"Greedy\":\n",
    "            generation_args[\"do_sample\"] = False\n",
    "        elif decoding_strategy == \"Top P Sampling\":\n",
    "            generation_args[\"temperature\"] = temperature\n",
    "            generation_args[\"do_sample\"] = True\n",
    "            generation_args[\"top_p\"] = top_p\n",
    "\n",
    "        generation_args.update(inputs)\n",
    "        return prompt, generation_args\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        img_or_path: Image.Image | Path | str,\n",
    "        prompt_text: str | None = None,\n",
    "        lang: str | None = None,\n",
    "        config: str | None = None,\n",
    "        show_prompt: bool = False,\n",
    "        **kwargs,\n",
    "    ) -> str:\n",
    "        if not self.is_idefics_available():\n",
    "            raise RuntimeError(\"Idefics is not installed or not found.\")\n",
    "        resulting_messages = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [{\"type\": \"image\"}] + [\n",
    "                    {\"type\": \"text\", \"text\": prompt_text or self.prompt_text_tmpl.format(lang or self.lang)}\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "        image = load_image(img_or_path)\n",
    "        prompt, generation_args = self._generation_args(image, resulting_messages)\n",
    "        generated_ids = self.MODEL.generate(**generation_args)\n",
    "        generated_texts = self.PROCESSOR.batch_decode(\n",
    "            generated_ids[:, generation_args[\"input_ids\"].size(1):], skip_special_tokens=True)\n",
    "        if show_prompt:\n",
    "            cprint(\"INPUT:\", prompt, \"|OUTPUT:\", generated_texts)\n",
    "        return generated_texts[0]#.strip('\"')\n",
    "\n",
    "    def postprocess_ocr(self, text):\n",
    "        return ' '.join(remove_multiple_whitespaces(text).splitlines())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IdeficsExperimentContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class IdeficsExperimentContext(OCRExperimentContext):\n",
    "    @functools.lru_cache()\n",
    "    def mocr(self, lang: str):\n",
    "        if self.ocr_model == 'Idefics':\n",
    "            proc = IdeficsOCR(lang)\n",
    "        else:\n",
    "            engine = self.engines[self.ocr_model]\n",
    "            ocr_processor = ocr.get_ocr_processor(True, engine)\n",
    "            proc = ocr_processor[lang2pcleaner(lang)]\n",
    "            if isinstance(proc, TesseractOcr):\n",
    "                proc.lang = lang2tesseract(lang)\n",
    "        return proc\n",
    "\n",
    "    def cleanup_model(self):\n",
    "        del IdeficsOCR.MODEL\n",
    "        torch.cuda.empty_cache()\n",
    "        import gc\n",
    "        gc.collect()\n",
    "        IdeficsOCR.MODEL = None\n",
    "\n",
    "    def setup_idefics(self, quant: QuantT = 'bfloat16', flashattn: bool = True):\n",
    "        if IdeficsOCR.PROCESSOR is None:\n",
    "            IdeficsOCR.setup_processor()\n",
    "        if IdeficsOCR.MODEL is not None:\n",
    "            self.cleanup_model()\n",
    "        if IdeficsOCR.MODEL is None:\n",
    "            IdeficsOCR.setup_model(quant=quant, flashattn=flashattn)\n",
    "\n",
    "    def show(self):\n",
    "        super().show()\n",
    "        cfg = IdeficsOCR.MODEL.config\n",
    "        if hasattr(cfg, 'quantization_config'):\n",
    "            qcfg = cfg.quantization_config\n",
    "            quant = '4bits' if qcfg.load_in_4bit else '8bits'\n",
    "        else:\n",
    "            quant = 'bfloat16'\n",
    "        cprint(\n",
    "            f\"{'Quantization':>17}: {quant!r}\\n\"\n",
    "            f\"{'Flash attention 2':>17}: {cfg._attn_implementation == 'flash_attention_2'}\\n\"\n",
    "            f\"{'VRAM':>17}: {get_gpu_vram(False)}/{get_gpu_vram()} MiB\\n\"\n",
    "        )\n",
    "\n",
    "    def __init__(self, \n",
    "            root_dir: Path | str | None = None, \n",
    "            quant: QuantT = 'bfloat16', \n",
    "            flashattn: bool = True,\n",
    "            *, \n",
    "            config: cfg.Config | None = None, \n",
    "            server: web_server.WebServer | None = None,\n",
    "            run_name: str = 'Idefics-crop-post', \n",
    "            setup_idefics: bool = True,\n",
    "        ):\n",
    "        super().__init__('Idefics', root_dir, config=config, server=server, run_name=run_name)\n",
    "        if setup_idefics:\n",
    "            self.setup_idefics(quant, flashattn)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTEXT = IdeficsExperimentContext(EXP_DIR)  # quantization 'bfloat16'  # Colab pro with A100 or L4, bfloat16 and FlashAttention\n",
    "# CONTEXT = IdeficsExperimentContext(EXP_DIR, '4bits', False)  # Free tier, T4 GPUs don't support FlashAttention\n",
    "# CONTEXT = IdeficsExperimentContext(EXP_DIR, '4bits')  # Linux Ampere\n",
    "CONTEXT.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE = IdeficsOCR.MODEL.device\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['00: Action_Comics_1960-01-00_(262).JPG',\n",
       " '01: Adolf_Cap_01_008.jpg',\n",
       " '02: Barnaby_v1-028.png',\n",
       " '03: Barnaby_v1-029.png',\n",
       " '04: Buck_Danny_-_12_-_Avions_Sans_Pilotes_-_013.jpg',\n",
       " '05: Cannon-292.jpg',\n",
       " '06: Contrato_con_Dios_028.jpg',\n",
       " '07: Erase_una_vez_en_Francia_02_88.jpg',\n",
       " '08: FOX_CHILLINTALES_T17_012.jpg',\n",
       " '09: Furari_-_Jiro_Taniguchi_selma_056.jpg',\n",
       " '10: Galactus_12.jpg',\n",
       " '11: INOUE_KYOUMEN_002.png',\n",
       " '12: MCCALL_ROBINHOOD_T31_010.jpg',\n",
       " '13: MCCAY_LITTLENEMO_090.jpg',\n",
       " '14: Mary_Perkins_On_Stage_v2006_1_-_P00068.jpg',\n",
       " '15: PIKE_BOYLOVEGIRLS_T41_012.jpg',\n",
       " '16: Sal_Buscema_Spaceknights_&_Superheroes_Ocular_Edition_1_1.png',\n",
       " '17: Sal_Buscema_Spaceknights_&_Superheroes_Ocular_Edition_1_1_K.png',\n",
       " '18: Sal_Buscema_Spaceknights_&_Superheroes_Ocular_Edition_1_2.png',\n",
       " '19: Spirou_Et_Fantasio_Integrale_06_1958_1959_0025_0024.jpg',\n",
       " '20: Strange_Tales_172005.jpg',\n",
       " '21: Strange_Tales_172021.jpg',\n",
       " '22: Tarzan_014-21.JPG',\n",
       " '23: Tintin_21_Les_Bijoux_de_la_Castafiore_page_39.jpg',\n",
       " '24: Transformers_-_Unicron_000-004.jpg',\n",
       " '25: Transformers_-_Unicron_000-016.jpg',\n",
       " '26: WARE_ACME_024.jpg',\n",
       " '27: Yoko_Tsuno_T01_1972-10.jpg',\n",
       " '28: Your_Name_Another_Side_Earthbound_T02_084.jpg',\n",
       " '29: manga_0033.jpg',\n",
       " '30: ronson-031.jpg',\n",
       " '31: 哀心迷図のバベル 第01巻 - 22002_00_059.jpg']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IMAGE_PATHS = CONTEXT.image_paths\n",
    "\n",
    "[f\"{i:02}: {_.name}\" for i,_ in enumerate(IMAGE_PATHS)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXP_RUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Idefics-crop-post'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EXP_RUN = CONTEXT.experiment_run()\n",
    "assert EXP_RUN is not None\n",
    "RUN_NAME = EXP_RUN.name\n",
    "RUN_NAME\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "812190efb87642a7af4d326f2cc60448",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output(layout=Layout(height='0px'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f2ec5840d264c33adf5893638aa3479",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(VBox(children=(Dropdown(index=20, layout=Layout(width='fit-content'), options={'Action_Comics_1…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecc28e9809964c069aeff8c06f57034d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "BASE_IMAGE_IDX: ImgIdT = cast(ImgIdT, CONTEXT.normalize_idx(\"Strange_Tales_172005.jpg\"))\n",
    "\n",
    "assert BASE_IMAGE_IDX is not None\n",
    "img_path = CONTEXT.final(CONTEXT.image_paths[BASE_IMAGE_IDX])\n",
    "assert img_path.exists()\n",
    "\n",
    "img_visor = ImageContextVisor(CONTEXT, BASE_IMAGE_IDX)\n",
    "img_visor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div id=\"wrapper-6c774e6c-94b0-4d49-bd70-c7e74436ebde\" style=\"width: 100%; max-height: 360px; overflow-y: auto;\">\n",
       "            <div id=\"6c774e6c-94b0-4d49-bd70-c7e74436ebde\" style=\"width: 100%;\"></div>\n",
       "            <script>\n",
       "                require([\"https://rawgit.com/caldwell/renderjson/master/renderjson.js\"], function() {\n",
       "                    renderjson.set_show_to_level(2);\n",
       "                    document.getElementById('6c774e6c-94b0-4d49-bd70-c7e74436ebde').appendChild(renderjson({\"image_path\": \"cache/Strange_Tales_172005/Strange_Tales_172005.png\", \"mask_path\": \"cache/Strange_Tales_172005/Strange_Tales_172005_mask.png\", \"original_path\": \"source/Strange_Tales_172005.jpg\", \"scale\": 1.0, \"blk_list\": [{\"xyxy\": [58, 64, 428, 208], \"lines\": [[[58, 62], [414, 64], [414, 88], [58, 86]], [[70, 86], [405, 86], [405, 108], [70, 108]], [[70, 106], [410, 106], [410, 129], [70, 129]], [[70, 127], [420, 127], [420, 149], [70, 149]], [[70, 147], [409, 147], [409, 169], [70, 169]], [[70, 167], [370, 167], [370, 190], [70, 190]], [[160, 187], [270, 187], [270, 210], [160, 210]]], \"vertical\": false, \"language\": \"eng\", \"font_size\": 21, \"distance\": [74.77837079135338, 96.77695264302439, 117.27459618921701, 137.76989231860426, 157.77504781898952, 178.29334863307128, 198.29803465009545], \"angle\": 0, \"vec\": [2130.0, 2.0], \"norm\": 2130.000938966929, \"merged\": false, \"weight\": 427.59999999999997, \"text\": [], \"prob\": 1, \"translation\": \"\", \"fg_r\": 0, \"fg_g\": 0, \"fg_b\": 0, \"bg_r\": 0, \"bg_g\": 0, \"bg_b\": 0, \"font_family\": \"\", \"bold\": false, \"underline\": false, \"italic\": false, \"alpha\": 255, \"rich_text\": \"\", \"line_spacing\": 1.0, \"_alignment\": -1, \"_target_lang\": \"\", \"_bounding_rect\": null, \"default_stroke_width\": 0.2, \"accumulate_color\": true}, {\"xyxy\": [477, 62, 850, 171], \"lines\": [[[480, 60], [823, 62], [823, 86], [480, 84]], [[480, 82], [842, 84], [842, 108], [480, 107]], [[477, 103], [825, 104], [825, 129], [477, 127]], [[477, 123], [825, 125], [825, 149], [477, 147]], [[493, 145], [807, 147], [807, 169], [493, 167]]], \"vertical\": false, \"language\": \"eng\", \"font_size\": 22, \"distance\": [69.58009146722691, 91.52993499754474, 112.5821232498025, 132.58184786003417, 153.5868064419293], \"angle\": 0, \"vec\": [1715.0, 9.0], \"norm\": 1715.0236149977643, \"merged\": false, \"weight\": 2407602.7, \"text\": [], \"prob\": 1, \"translation\": \"\", \"fg_r\": 0, \"fg_g\": 0, \"fg_b\": 0, \"bg_r\": 0, \"bg_g\": 0, \"bg_b\": 0, \"font_family\": \"\", \"bold\": false, \"underline\": false, \"italic\": false, \"alpha\": 255, \"rich_text\": \"\", \"line_spacing\": 1.0, \"_alignment\": -1, \"_target_lang\": \"\", \"_bounding_rect\": null, \"default_stroke_width\": 0.2, \"accumulate_color\": true}, {\"xyxy\": [481, 303, 666, 392], \"lines\": [[[501, 305], [626, 305], [626, 328], [501, 328]], [[486, 324], [650, 324], [650, 346], [486, 346]], [[490, 344], [637, 346], [637, 370], [490, 368]], [[486, 364], [654, 366], [654, 391], [486, 389]]], \"vertical\": false, \"language\": \"eng\", \"font_size\": 22, \"distance\": [312.76135349492733, 331.23114715329416, 353.2604654051972, 373.7169704646074], \"angle\": 0, \"vec\": [604.0, 4.0], \"norm\": 604.0132448878915, \"merged\": false, \"weight\": 2407725.7, \"text\": [], \"prob\": 1, \"translation\": \"\", \"fg_r\": 0, \"fg_g\": 0, \"fg_b\": 0, \"bg_r\": 0, \"bg_g\": 0, \"bg_b\": 0, \"font_family\": \"\", \"bold\": false, \"underline\": false, \"italic\": false, \"alpha\": 255, \"rich_text\": \"\", \"line_spacing\": 1.0, \"_alignment\": -1, \"_target_lang\": \"\", \"_bounding_rect\": null, \"default_stroke_width\": 0.2, \"accumulate_color\": true}, {\"xyxy\": [701, 224, 831, 289], \"lines\": [[[722, 226], [821, 226], [821, 249], [722, 249]], [[707, 246], [821, 246], [821, 269], [707, 269]], [[711, 267], [814, 267], [814, 291], [711, 291]]], \"vertical\": false, \"language\": \"eng\", \"font_size\": 21, \"distance\": [237.49999999999994, 257.50000000000006, 279.00000000000034], \"angle\": 0, \"vec\": [316.0, 0.0], \"norm\": 316.0, \"merged\": false, \"weight\": 2407865.7, \"text\": [], \"prob\": 1, \"translation\": \"\", \"fg_r\": 0, \"fg_g\": 0, \"fg_b\": 0, \"bg_r\": 0, \"bg_g\": 0, \"bg_b\": 0, \"font_family\": \"\", \"bold\": false, \"underline\": false, \"italic\": false, \"alpha\": 255, \"rich_text\": \"\", \"line_spacing\": 1.0, \"_alignment\": -1, \"_target_lang\": \"\", \"_bounding_rect\": null, \"default_stroke_width\": 0.2, \"accumulate_color\": true}, {\"xyxy\": [1004, 67, 1214, 190], \"lines\": [[[1007, 66], [1190, 66], [1190, 88], [1007, 88]], [[1009, 88], [1195, 88], [1195, 105], [1009, 105]], [[1024, 106], [1182, 106], [1182, 129], [1024, 129]], [[1007, 125], [1204, 125], [1204, 147], [1007, 147]], [[1007, 147], [1206, 147], [1206, 169], [1007, 169]], [[1057, 165], [1155, 167], [1155, 191], [1057, 190]]], \"vertical\": false, \"language\": \"eng\", \"font_size\": 20, \"distance\": [75.3860596817718, 94.88089662537139, 115.87940481594718, 134.375711985048, 156.37421909642663, 176.37493208566144], \"angle\": 0, \"vec\": [1021.0, 1.5], \"norm\": 1021.0011018603261, \"merged\": false, \"weight\": 4814839.3, \"text\": [], \"prob\": 1, \"translation\": \"\", \"fg_r\": 0, \"fg_g\": 0, \"fg_b\": 0, \"bg_r\": 0, \"bg_g\": 0, \"bg_b\": 0, \"font_family\": \"\", \"bold\": false, \"underline\": false, \"italic\": false, \"alpha\": 255, \"rich_text\": \"\", \"line_spacing\": 1.0, \"_alignment\": -1, \"_target_lang\": \"\", \"_bounding_rect\": null, \"default_stroke_width\": 0.2, \"accumulate_color\": true}, {\"xyxy\": [95, 688, 304, 756], \"lines\": [[[95, 689], [302, 691], [302, 715], [95, 713]], [[106, 709], [300, 711], [300, 735], [106, 733]], [[141, 733], [265, 733], [265, 756], [141, 756]]], \"vertical\": false, \"language\": \"eng\", \"font_size\": 22, \"distance\": [700.4672883193294, 720.4324231265472, 742.9317700937542], \"angle\": 0, \"vec\": [525.0, 4.0], \"norm\": 525.0152378741021, \"merged\": false, \"weight\": 7222089.4, \"text\": [], \"prob\": 1, \"translation\": \"\", \"fg_r\": 0, \"fg_g\": 0, \"fg_b\": 0, \"bg_r\": 0, \"bg_g\": 0, \"bg_b\": 0, \"font_family\": \"\", \"bold\": false, \"underline\": false, \"italic\": false, \"alpha\": 255, \"rich_text\": \"\", \"line_spacing\": 1.0, \"_alignment\": -1, \"_target_lang\": \"\", \"_bounding_rect\": null, \"default_stroke_width\": 0.2, \"accumulate_color\": true}, {\"xyxy\": [42, 793, 235, 919], \"lines\": [[[44, 796], [224, 796], [224, 818], [44, 818]], [[58, 818], [204, 818], [204, 835], [58, 835]], [[49, 838], [226, 838], [226, 855], [49, 855]], [[55, 857], [210, 857], [210, 879], [55, 879]], [[68, 879], [189, 879], [189, 901], [68, 901]], [[66, 897], [186, 897], [186, 920], [66, 920]]], \"vertical\": false, \"language\": \"eng\", \"font_size\": 18, \"distance\": [807.0, 826.5, 846.5, 867.9999999999999, 890.0, 908.5], \"angle\": 0, \"vec\": [899.0, 0.0], \"norm\": 899.0, \"merged\": false, \"weight\": 7222150.2, \"text\": [], \"prob\": 1, \"translation\": \"\", \"fg_r\": 0, \"fg_g\": 0, \"fg_b\": 0, \"bg_r\": 0, \"bg_g\": 0, \"bg_b\": 0, \"font_family\": \"\", \"bold\": false, \"underline\": false, \"italic\": false, \"alpha\": 255, \"rich_text\": \"\", \"line_spacing\": 1.0, \"_alignment\": -1, \"_target_lang\": \"\", \"_bounding_rect\": null, \"default_stroke_width\": 0.2, \"accumulate_color\": true}, {\"xyxy\": [533, 803, 702, 932], \"lines\": [[[536, 807], [685, 807], [685, 829], [536, 829]], [[538, 831], [692, 831], [692, 848], [538, 848]], [[538, 851], [665, 851], [665, 868], [538, 868]], [[534, 868], [666, 868], [666, 890], [534, 890]], [[539, 890], [639, 890], [639, 912], [539, 912]], [[556, 910], [615, 910], [615, 934], [556, 934]]], \"vertical\": false, \"language\": \"eng\", \"font_size\": 19, \"distance\": [818.0, 839.5, 859.5, 879.0, 901.0, 922.0000000000001], \"angle\": 0, \"vec\": [721.0, 0.0], \"norm\": 721.0, \"merged\": false, \"weight\": 9629426.5, \"text\": [], \"prob\": 1, \"translation\": \"\", \"fg_r\": 0, \"fg_g\": 0, \"fg_b\": 0, \"bg_r\": 0, \"bg_g\": 0, \"bg_b\": 0, \"font_family\": \"\", \"bold\": false, \"underline\": false, \"italic\": false, \"alpha\": 255, \"rich_text\": \"\", \"line_spacing\": 1.0, \"_alignment\": -1, \"_target_lang\": \"\", \"_bounding_rect\": null, \"default_stroke_width\": 0.2, \"accumulate_color\": true}, {\"xyxy\": [949, 800, 1181, 921], \"lines\": [[[954, 800], [1146, 800], [1146, 822], [954, 822]], [[952, 820], [1151, 820], [1151, 842], [952, 842]], [[956, 840], [1168, 840], [1168, 863], [956, 863]], [[963, 860], [1162, 860], [1162, 883], [963, 883]], [[989, 881], [1123, 881], [1123, 903], [989, 903]], [[1011, 901], [1125, 901], [1125, 923], [1011, 923]]], \"vertical\": false, \"language\": \"eng\", \"font_size\": 20, \"distance\": [811.0000000000003, 830.9999999999998, 851.5000000000002, 871.5000000000002, 892.0000000000001, 912.0000000000001], \"angle\": 0, \"vec\": [1050.0, 0.0], \"norm\": 1050.0, \"merged\": false, \"weight\": 12036646.5, \"text\": [], \"prob\": 1, \"translation\": \"\", \"fg_r\": 0, \"fg_g\": 0, \"fg_b\": 0, \"bg_r\": 0, \"bg_g\": 0, \"bg_b\": 0, \"font_family\": \"\", \"bold\": false, \"underline\": false, \"italic\": false, \"alpha\": 255, \"rich_text\": \"\", \"line_spacing\": 1.0, \"_alignment\": -1, \"_target_lang\": \"\", \"_bounding_rect\": null, \"default_stroke_width\": 0.2, \"accumulate_color\": true}, {\"xyxy\": [342, 1164, 461, 1248], \"lines\": [[[386, 1165], [410, 1165], [410, 1183], [386, 1183]], [[346, 1181], [455, 1181], [455, 1205], [346, 1205]], [[350, 1203], [451, 1203], [451, 1226], [350, 1226]], [[351, 1224], [445, 1224], [445, 1246], [351, 1246]]], \"vertical\": false, \"language\": \"eng\", \"font_size\": 20, \"distance\": [1174.0, 1192.9999999999998, 1214.5, 1235.0], \"angle\": 0, \"vec\": [328.0, 0.0], \"norm\": 328.0, \"merged\": false, \"weight\": 14443943.8, \"text\": [], \"prob\": 1, \"translation\": \"\", \"fg_r\": 0, \"fg_g\": 0, \"fg_b\": 0, \"bg_r\": 0, \"bg_g\": 0, \"bg_b\": 0, \"font_family\": \"\", \"bold\": false, \"underline\": false, \"italic\": false, \"alpha\": 255, \"rich_text\": \"\", \"line_spacing\": 1.0, \"_alignment\": -1, \"_target_lang\": \"\", \"_bounding_rect\": null, \"default_stroke_width\": 0.2, \"accumulate_color\": true}, {\"xyxy\": [967, 939, 1175, 1006], \"lines\": [[[967, 942], [1166, 942], [1166, 964], [967, 964]], [[983, 962], [1169, 962], [1169, 984], [983, 984]], [[978, 982], [1153, 982], [1153, 1005], [978, 1005]]], \"vertical\": false, \"language\": \"eng\", \"font_size\": 20, \"distance\": [952.9999999999998, 973.0000000000001, 993.5000000000001], \"angle\": 0, \"vec\": [560.0, 0.0], \"norm\": 560.0, \"merged\": false, \"weight\": 19257893.7, \"text\": [], \"prob\": 1, \"translation\": \"\", \"fg_r\": 0, \"fg_g\": 0, \"fg_b\": 0, \"bg_r\": 0, \"bg_g\": 0, \"bg_b\": 0, \"font_family\": \"\", \"bold\": false, \"underline\": false, \"italic\": false, \"alpha\": 255, \"rich_text\": \"\", \"line_spacing\": 1.0, \"_alignment\": -1, \"_target_lang\": \"\", \"_bounding_rect\": null, \"default_stroke_width\": 0.2, \"accumulate_color\": true}, {\"xyxy\": [69, 1419, 412, 1502], \"lines\": [[[70, 1417], [412, 1417], [412, 1440], [70, 1440]], [[79, 1439], [401, 1439], [401, 1462], [79, 1462]], [[79, 1458], [407, 1458], [407, 1480], [79, 1480]], [[208, 1480], [259, 1480], [259, 1502], [208, 1502]]], \"vertical\": false, \"language\": \"eng\", \"font_size\": 20, \"distance\": [1428.5, 1450.5, 1469.0, 1491.0], \"angle\": 0, \"vec\": [1043.0, 0.0], \"norm\": 1043.0, \"merged\": false, \"weight\": 21665133.1, \"text\": [], \"prob\": 1, \"translation\": \"\", \"fg_r\": 0, \"fg_g\": 0, \"fg_b\": 0, \"bg_r\": 0, \"bg_g\": 0, \"bg_b\": 0, \"font_family\": \"\", \"bold\": false, \"underline\": false, \"italic\": false, \"alpha\": 255, \"rich_text\": \"\", \"line_spacing\": 1.0, \"_alignment\": -1, \"_target_lang\": \"\", \"_bounding_rect\": null, \"default_stroke_width\": 0.2, \"accumulate_color\": true}, {\"xyxy\": [472, 1420, 724, 1484], \"lines\": [[[477, 1419], [711, 1419], [711, 1441], [477, 1441]], [[475, 1437], [714, 1439], [714, 1462], [475, 1460]], [[479, 1456], [690, 1460], [690, 1484], [479, 1480]]], \"vertical\": false, \"language\": \"eng\", \"font_size\": 21, \"distance\": [1424.7346603857238, 1444.2295244019977, 1464.816451666877], \"angle\": 0, \"vec\": [684.0, 6.0], \"norm\": 684.0263152832645, \"merged\": false, \"weight\": 24072243.6, \"text\": [], \"prob\": 1, \"translation\": \"\", \"fg_r\": 0, \"fg_g\": 0, \"fg_b\": 0, \"bg_r\": 0, \"bg_g\": 0, \"bg_b\": 0, \"font_family\": \"\", \"bold\": false, \"underline\": false, \"italic\": false, \"alpha\": 255, \"rich_text\": \"\", \"line_spacing\": 1.0, \"_alignment\": -1, \"_target_lang\": \"\", \"_bounding_rect\": null, \"default_stroke_width\": 0.2, \"accumulate_color\": true}, {\"xyxy\": [799, 1758, 982, 1822], \"lines\": [[[810, 1760], [967, 1760], [967, 1783], [810, 1783]], [[801, 1780], [974, 1779], [974, 1801], [801, 1803]], [[808, 1801], [969, 1801], [969, 1823], [808, 1823]]], \"vertical\": false, \"language\": \"eng\", \"font_size\": 20, \"distance\": [1774.2060791718054, 1793.2029355337188, 1814.705890180632], \"angle\": 0, \"vec\": [491.0, -1.5], \"norm\": 491.00229123701655, \"merged\": false, \"weight\": 26479622.6, \"text\": [], \"prob\": 1, \"translation\": \"\", \"fg_r\": 0, \"fg_g\": 0, \"fg_b\": 0, \"bg_r\": 0, \"bg_g\": 0, \"bg_b\": 0, \"font_family\": \"\", \"bold\": false, \"underline\": false, \"italic\": false, \"alpha\": 255, \"rich_text\": \"\", \"line_spacing\": 1.0, \"_alignment\": -1, \"_target_lang\": \"\", \"_bounding_rect\": null, \"default_stroke_width\": 0.2, \"accumulate_color\": true}, {\"xyxy\": [858, 1827, 1238, 1860], \"lines\": [[[877, 1830], [1007, 1830], [1007, 1854], [877, 1854]]], \"vertical\": false, \"language\": \"eng\", \"font_size\": 22, \"distance\": [1842.0], \"angle\": 0, \"vec\": [130.0, 0.0], \"norm\": 130.0, \"merged\": false, \"weight\": 26479865.1, \"text\": [], \"prob\": 1, \"translation\": \"\", \"fg_r\": 0, \"fg_g\": 0, \"fg_b\": 0, \"bg_r\": 0, \"bg_g\": 0, \"bg_b\": 0, \"font_family\": \"\", \"bold\": false, \"underline\": false, \"italic\": false, \"alpha\": 255, \"rich_text\": \"\", \"line_spacing\": 1.0, \"_alignment\": -1, \"_target_lang\": \"\", \"_bounding_rect\": null, \"default_stroke_width\": 0.2, \"accumulate_color\": true}]}));\n",
       "                });\n",
       "            </script>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "page_lang = 'English'\n",
    "\n",
    "IMAGE_CONTEXT = ImageContext(CONTEXT, BASE_IMAGE_IDX, page_lang=page_lang)\n",
    "test_eq(IMAGE_CONTEXT.page_data is not None, True)\n",
    "RenderJSON(IMAGE_CONTEXT.json_data, 360, 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Box id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOX_IDX = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Idefics inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_lang = IMAGE_CONTEXT.page_lang\n",
    "\n",
    "resulting_messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [{\"type\": \"image\"}] + [\n",
    "            {\"type\": \"text\", \"text\": prompt_text_tmpl.format(page_lang)}\n",
    "        ]\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def idefics_generation_args(image: Image.Image, resulting_messages: list[dict]):\n",
    "    processor = IdeficsOCR.PROCESSOR\n",
    "    prompt = processor.apply_chat_template(resulting_messages, add_generation_prompt=True)\n",
    "    inputs = processor(text=prompt, images=[image], return_tensors=\"pt\")\n",
    "    inputs = {k: v.to(DEVICE) for k, v in inputs.items()}\n",
    "    \n",
    "    max_new_tokens = 512\n",
    "    repetition_penalty = 1.2\n",
    "    decoding_strategy = \"Greedy\"\n",
    "    temperature = 0.4\n",
    "    top_p = 0.8\n",
    "\n",
    "    generation_args = {\n",
    "        \"max_new_tokens\": max_new_tokens,\n",
    "        \"repetition_penalty\": repetition_penalty,\n",
    "    }\n",
    "\n",
    "    assert decoding_strategy in [\n",
    "        \"Greedy\",\n",
    "        \"Top P Sampling\",\n",
    "    ]\n",
    "\n",
    "    if decoding_strategy == \"Greedy\":\n",
    "        generation_args[\"do_sample\"] = False\n",
    "    elif decoding_strategy == \"Top P Sampling\":\n",
    "        generation_args[\"temperature\"] = temperature\n",
    "        generation_args[\"do_sample\"] = True\n",
    "        generation_args[\"top_p\"] = top_p\n",
    "\n",
    "    generation_args.update(inputs)\n",
    "    return prompt, generation_args\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crop methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_experiment = ExperimentOCR.from_image(CONTEXT, RUN_NAME, IMAGE_CONTEXT.image_idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "method = CropMethod.INITIAL_BOX\n",
    "\n",
    "result = cast(ResultOCR, image_experiment.result(BOX_IDX, method, ocr=False))\n",
    "image = cast(Image.Image, result.image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">INPUT: User:<span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">image</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;Please perform optical character recognition </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #000000; text-decoration-color: #000000\">OCR</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\"> on this image, which displays </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">speech balloons from a comic book. The text is in English. Extract the text and format it as follows: </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">transcribe in standard sentence case, capitalized. Avoid using all capital letters, but ensure it is </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">capitalized where appropriate, including proper nouns. Provide the transcribed text clearly. Double </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">check the text is not all capital letters.&lt;end_of_utterance</span><span style=\"font-weight: bold\">&gt;</span>\n",
       "Assistant: |OUTPUT:\n",
       "<span style=\"font-weight: bold\">[</span>\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'Embodied by great gnarled cypress trees, the ancient manor stands alone on the outskirts of New </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Orleans, kept tidy by a white-haired old man known only as Bambu.'</span>\n",
       "<span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "INPUT: User:\u001b[1m<\u001b[0m\u001b[1;95mimage\u001b[0m\u001b[39m>Please perform optical character recognition \u001b[0m\u001b[1;39m(\u001b[0m\u001b[39mOCR\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m on this image, which displays \u001b[0m\n",
       "\u001b[39mspeech balloons from a comic book. The text is in English. Extract the text and format it as follows: \u001b[0m\n",
       "\u001b[39mtranscribe in standard sentence case, capitalized. Avoid using all capital letters, but ensure it is \u001b[0m\n",
       "\u001b[39mcapitalized where appropriate, including proper nouns. Provide the transcribed text clearly. Double \u001b[0m\n",
       "\u001b[39mcheck the text is not all capital letters.<end_of_utterance\u001b[0m\u001b[1m>\u001b[0m\n",
       "Assistant: |OUTPUT:\n",
       "\u001b[1m[\u001b[0m\n",
       "    \u001b[32m'Embodied by great gnarled cypress trees, the ancient manor stands alone on the outskirts of New \u001b[0m\n",
       "\u001b[32mOrleans, kept tidy by a white-haired old man known only as Bambu.'\u001b[0m\n",
       "\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt, generation_args = idefics_generation_args(image, resulting_messages)\n",
    "generated_ids = IdeficsOCR.MODEL.generate(**generation_args)\n",
    "\n",
    "generated_texts = IdeficsOCR.PROCESSOR.batch_decode(\n",
    "    generated_ids[:, generation_args[\"input_ids\"].size(1):], skip_special_tokens=True)\n",
    "cprint(\"INPUT:\", prompt, \"|OUTPUT:\", generated_texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><colgroup><col style='width: 370px;'/><col style='width: auto;'/></colgroup><tr><td style='text-align: center;'><img src=\"../experiment/cache/Strange_Tales_172005/.crop/Strange_Tales_172005_0_Initial box.png\"/></td><td style='font-size: 12pt; text-align: left; '>Embodied by great gnarled cypress trees, the ancient manor stands alone on the outskirts of New Orleans, kept tidy by a white-haired old man known only as Bambu.<br/><strong><span style='color: red;'>0.98</span></strong></td></tr></table>\n",
       "<br/>\n",
       "<pre style='font-size: 14px;'><div style='font-family: monospace; white-space: pre-wrap;'>Embowered by great gnarled cypress trees, the ancient manor stands alone on the outskirts of New Orleans, kept tidy by a white-haired old man known only as Bambu.</div><br/><div style='font-family: monospace; white-space: pre-wrap;'>Embo<span style='color: red;'>di</span><span style='color: red;'>&#x2395;</span>ed by great gnarled cypress trees, the ancient manor stands alone on the outskirts of New Orleans, kept tidy by a white-haired old man known only as Bambu.</div></pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "result.ocr = generated_texts[0]\n",
    "result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">INPUT: User:<span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">image</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;Please perform optical character recognition </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #000000; text-decoration-color: #000000\">OCR</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\"> on this image, which contains </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">speech balloons from a comic book. The text is in English. Carefully transcribe the text, ensuring that </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">you preserve the original formatting and line breaks as they appear in the speech </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">balloon.&lt;end_of_utterance</span><span style=\"font-weight: bold\">&gt;</span>\n",
       "Assistant: |OUTPUT:\n",
       "<span style=\"font-weight: bold\">[</span>\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'EMBODIED BY GREAT GARLLED CYPRESS TREES, THE ANCIENT MANOR STANDS ALONE ON THE OUTSKIRTS OF NEW </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">ORLEANS, KEPT TIDY BY A WHITE-HAIRING OLD MAN KNOWN ONLY AS BAMBU.'</span>\n",
       "<span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "INPUT: User:\u001b[1m<\u001b[0m\u001b[1;95mimage\u001b[0m\u001b[39m>Please perform optical character recognition \u001b[0m\u001b[1;39m(\u001b[0m\u001b[39mOCR\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m on this image, which contains \u001b[0m\n",
       "\u001b[39mspeech balloons from a comic book. The text is in English. Carefully transcribe the text, ensuring that \u001b[0m\n",
       "\u001b[39myou preserve the original formatting and line breaks as they appear in the speech \u001b[0m\n",
       "\u001b[39mballoon.<end_of_utterance\u001b[0m\u001b[1m>\u001b[0m\n",
       "Assistant: |OUTPUT:\n",
       "\u001b[1m[\u001b[0m\n",
       "    \u001b[32m'EMBODIED BY GREAT GARLLED CYPRESS TREES, THE ANCIENT MANOR STANDS ALONE ON THE OUTSKIRTS OF NEW \u001b[0m\n",
       "\u001b[32mORLEANS, KEPT TIDY BY A WHITE-HAIRING OLD MAN KNOWN ONLY AS BAMBU.'\u001b[0m\n",
       "\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table><colgroup><col style='width: 370px;'/><col style='width: auto;'/></colgroup><tr><td style='text-align: center;'><img src=\"../experiment/cache/Strange_Tales_172005/.crop/Strange_Tales_172005_0_Initial box.png\"/></td><td style='font-size: 12pt; text-align: left; '>EMBODIED BY GREAT GARLLED CYPRESS TREES, THE ANCIENT MANOR STANDS ALONE ON THE OUTSKIRTS OF NEW ORLEANS, KEPT TIDY BY A WHITE-HAIRING OLD MAN KNOWN ONLY AS BAMBU.<br/><strong><span style='color: red;'>0.03</span></strong></td></tr></table>\n",
       "<br/>\n",
       "<pre style='font-size: 14px;'><div style='font-family: monospace; white-space: pre-wrap;'>Embowered by great gnarled cypress trees, the ancient manor stands alone on the outskirts of New Orleans, kept tidy by a white-haired old man known only as Bambu<span style='color: green;'>&#x2395;&#x2395;&#x2395;&#x2395;&#x2395;&#x2395;&#x2395;&#x2395;&#x2395;&#x2395;&#x2395;&#x2395;&#x2395;&#x2395;&#x2395;&#x2395;&#x2395;&#x2395;&#x2395;&#x2395;&#x2395;&#x2395;&#x2395;&#x2395;&#x2395;&#x2395;&#x2395;&#x2395;&#x2395;&#x2395;&#x2395;&#x2395;&#x2395;&#x2395;&#x2395;&#x2395;&#x2395;&#x2395;&#x2395;&#x2395;&#x2395;</span>.</div><br/><div style='font-family: monospace; white-space: pre-wrap;'>E<span style='color: red;'>MBODIED</span><span style='color: red;'>&#x2395;</span> <span style='color: red;'>BY</span> <span style='color: red;'>GREAT</span> <span style='color: red;'>GARLLED</span> <span style='color: red;'>CYPRESS</span> <span style='color: red;'>TREES</span>, <span style='color: red;'>THE</span> <span style='color: red;'>ANCIENT</span> <span style='color: red;'>MANOR</span> <span style='color: red;'>STANDS</span> <span style='color: red;'>ALONE</span> <span style='color: red;'>ON</span> <span style='color: red;'>THE</span> <span style='color: red;'>OUTSKIRTS</span> <span style='color: red;'>OF</span> N<span style='color: red;'>EW</span> O<span style='color: red;'>RLEANS</span>, <span style='color: red;'>KEPT</span> <span style='color: red;'>TIDY</span><span style='color: red;'>&#x2395;&#x2395;&#x2395;&#x2395;&#x2395;&#x2395;&#x2395;&#x2395;&#x2395;&#x2395;&#x2395;&#x2395;&#x2395;&#x2395;&#x2395;&#x2395;&#x2395;&#x2395;&#x2395;&#x2395;&#x2395;&#x2395;&#x2395;&#x2395;&#x2395;&#x2395;&#x2395;&#x2395;&#x2395;&#x2395;&#x2395;&#x2395;&#x2395;&#x2395;&#x2395;&#x2395;&#x2395;&#x2395;&#x2395;&#x2395;</span> B<span style='color: red;'>Y A WHITE-HAIRING OLD MAN KNOWN ONLY AS BAMBU</span>.</div></pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "method = CropMethod.INITIAL_BOX\n",
    "\n",
    "result = cast(ResultOCR, image_experiment.result(BOX_IDX, method, ocr=False))\n",
    "image = cast(Image.Image, result.image)\n",
    "\n",
    "mocr: IdeficsOCR = cast(IdeficsOCR, CONTEXT.mocr(page_lang))\n",
    "text = mocr(image, show_prompt=True)\n",
    "result.ocr = mocr.postprocess_ocr(text)\n",
    "result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><colgroup><col style='width: 378px;'/><col style='width: auto;'/></colgroup><tr><td style='text-align: center;'><img src=\"../experiment/cache/Strange_Tales_172005/.crop/Strange_Tales_172005_0_Padded 4px.png\"/></td><td style='font-size: 12pt; text-align: left; '>Embowered by great gnarled cypress trees, the ancient manor stands alone on the outskirts of new orleans, kept tidy by a white-haired old man known only as bambu.<br/><strong><span style='color: red;'>0.98</span></strong></td></tr></table>\n",
       "<br/>\n",
       "<pre style='font-size: 14px;'><div style='font-family: monospace; white-space: pre-wrap;'>Embowered by great gnarled cypress trees, the ancient manor stands alone on the outskirts of New Orleans, kept tidy by a white-haired old man known only as Bambu.</div><br/><div style='font-family: monospace; white-space: pre-wrap;'>Embowered by great gnarled cypress trees, the ancient manor stands alone on the outskirts of <span style='color: red;'>n</span>ew <span style='color: red;'>o</span>rleans, kept tidy by a white-haired old man known only as <span style='color: red;'>b</span>ambu.</div></pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image_experiment.result(BOX_IDX, CropMethod.PADDED_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr><td style='text-align: center;'><img src=\"../experiment/cache/Strange_Tales_172005/.crop/Strange_Tales_172005_0_Pad 8, fract. 0.2_cropped.png\"/></td><td style='text-align: center;'><img src=\"../experiment/cache/Strange_Tales_172005/.crop/Strange_Tales_172005_0_Pad 8, fract. 0.2_mask.png\"/></td><td style='text-align: center;'><img src=\"../experiment/cache/Strange_Tales_172005/.crop/Strange_Tales_172005_0_Pad 8, fract. 0.2.png\"/></td></tr></table>\n",
       "<br/>\n",
       "<div style='font-size: 12pt;'><strong style='color: red;'>0.94</strong><div/>\n",
       "<br/>\n",
       "<pre style='font-size: 14px;'><div style='font-family: monospace; white-space: pre-wrap;'>Embow<span style='color: green;'>&#x2395;&#x2395;</span>ered by great gnarled cypress trees, the ancient manor stands alone on the outskirts of New Orleans, kept tidy by a white-haired old man known only as Bambu.</div><br/><div style='font-family: monospace; white-space: pre-wrap;'>E<span style='color: red;'>nc</span>o<span style='color: red;'>unt</span>ered by great <span style='color: red;'>ch</span>arle<span style='color: red;'>s</span> cypress trees, the ancient manor stands alone on the outskirts of <span style='color: red;'>n</span>ew <span style='color: red;'>o</span>rleans, kept tidy by a white-haired old man known only as <span style='color: red;'>b</span>ambu.</div></pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image_experiment.result(BOX_IDX, CropMethod.PAD_8_FRACT_0_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# Visualize results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ece4fd54dda74827b08fd4be6361f999",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output(layout=Layout(height='0px'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b321b562b114cc9b7291e4eacc95369",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HBox(children=(Label(value='Box # (of 15):', layout=Layout(padding='0px 0px 0px 10px', width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d5be8ec51df436d84a51e239598876e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "result_visor = ResultVisor(image_experiment)\n",
    "result_visor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# Visualize Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecb2d6523b5c468c81a7d11430d9ba27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(VBox(children=(Dropdown(index=20, layout=Layout(width='fit-content'), options={'Action_Comics_1…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5344e244968341469b4298bb2217171a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "exp_visor = ExperimentVisor(image_experiment)\n",
    "exp_visor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# EEAaO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9638924083d466a80eca965c470aad4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(HBox(children=(HBox(children=(Dropdown(layout=Layout(width='fit-content'), optio…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3acd3a8c49a54d7c9c2488247626166b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "idefics_experiment = ExperimentsVisor(\n",
    "                        CONTEXT, \n",
    "                        'Idefics', \n",
    "                        image_idx=BASE_IMAGE_IDX, \n",
    "                        box_idx=13, \n",
    "                        method=CropMethod.DEFAULT_GREY_PAD\n",
    "                    )\n",
    "idefics_experiment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Colophon\n",
    "----\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fastcore.all as FC\n",
    "from nbdev.export import nb_export\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if FC.IN_NOTEBOOK:\n",
    "    nb_export('ocr_idefics.ipynb', '..')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
