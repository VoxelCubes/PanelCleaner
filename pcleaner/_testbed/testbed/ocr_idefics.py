# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/ocr_idefics.ipynb.

# %% ../nbs/ocr_idefics.ipynb 1
from __future__ import annotations


# %% auto 0
__all__ = ['IdeficsOCR', 'IdeficsExperimentContext']

# %% ../nbs/ocr_idefics.ipynb 5
import functools
import subprocess
from pathlib import Path
from typing import Any
from typing import Literal
from typing import TypeAlias

import pcleaner.config as cfg
import pcleaner.ocr.ocr as ocr
import torch
from pcleaner.ocr.ocr_tesseract import TesseractOcr
from PIL import Image
from rich.console import Console
from transformers import AutoProcessor
from transformers import Idefics2ForConditionalGeneration


# %% ../nbs/ocr_idefics.ipynb 14
console = Console(width=104, tab_size=4, force_jupyter=True)
cprint = console.print


# %% ../nbs/ocr_idefics.ipynb 17
import pcleaner._testbed.testbed.experiments as exp_testbed
from pcleaner._testbed.testbed.experiments import *
from pcleaner._testbed.testbed.helpers import RenderJSON
import pcleaner._testbed.testbed.web_server as web_server


# %% ../nbs/ocr_idefics.ipynb 18
def load_image(img_or_path) -> Image.Image:
    if isinstance(img_or_path, (str, Path)):
        return Image.open(img_or_path)
    elif isinstance(img_or_path, Image.Image):
        return img_or_path
    else:
        raise ValueError(f"img_or_path must be a path or PIL.Image, got: {type(img_or_path)}")


# %% ../nbs/ocr_idefics.ipynb 19
def get_gpu_vram(total=True):
    if total:
        command = "nvidia-smi --query-gpu=memory.total --format=csv,noheader,nounits"
    else:
        command = "nvidia-smi --query-gpu=memory.used --format=csv,noheader,nounits"
    try:
        vram = subprocess.check_output(command, shell=True).decode('utf-8').strip()
        return vram
    except subprocess.CalledProcessError:
        return "Failed to get VRAM"


# %% ../nbs/ocr_idefics.ipynb 43
def _setup_processor():
    return AutoProcessor.from_pretrained(
        "HuggingFaceM4/idefics2-8b", 
        do_image_splitting=False  #  cropped boxes are usually small
        )

# %% ../nbs/ocr_idefics.ipynb 45
QuantT: TypeAlias = Literal['bfloat16'] | Literal['8bits'] | Literal['4bits']

def _setup_model(quant: QuantT, flashattn: bool=True):
    kwargs: dict = dict(
        torch_dtype=torch.bfloat16,
    )
    if quant == 'bfloat16':
        pass
    else:
        from transformers import BitsAndBytesConfig
        quantization_config = None
        if quant == '8bits':
            quantization_config = BitsAndBytesConfig(
                load_in_8bit=True,
            )
        if quant == '4bits':
            quantization_config = BitsAndBytesConfig(
                load_in_4bit=True,
                bnb_4bit_quant_type="nf4",
                bnb_4bit_use_double_quant=True,
                bnb_4bit_compute_dtype=torch.float16
            )
        if quantization_config is not None:
            kwargs.update(quantization_config=quantization_config)
    if flashattn:
        kwargs.update(_attn_implementation="flash_attention_2")
    model = Idefics2ForConditionalGeneration.from_pretrained(
        "HuggingFaceM4/idefics2-8b", 
        device_map='auto', 
        **kwargs)
    return model

# %% ../nbs/ocr_idefics.ipynb 46
prompt_text_tmpl = (
        "Please perform optical character recognition (OCR) on this image, which displays "
        "speech balloons from a comic book. The text is in {}. Extract the text and "
        "format it as follows: transcribe in standard sentence case, avoid using all capital "
        "letters. Provide the transcribed text clearly and double check the sentence is not all capital letters.")

# prompt_text_tmpl = ("Please perform optical character recognition (OCR) on this image, which displays "
#         f"speech balloons from a manga comic. The text is in {}. Extract the text and "
#         "format it without newlines. Provide the transcribed text clearly.")

# prompt_text_tmpl = ("Please perform optical character recognition (OCR) on this image, which displays "
#         "speech balloons from a comic book. The text is in {}. Extract the text and "
#         "format it as follows: transcribe in standard sentence case (avoid using all capital "
#         "letters) and use asterisks to denote any words that appear in bold within the image. "
#         "Provide the transcribed text clearly.")

# prompt_text_tmpl = ("Please perform optical character recognition (OCR) on this image, which displays "
#         "speech balloons from a comic book. The text is in {}. Extract the text and "
#         "format it as follows: transcribe in standard sentence case, capitalized. Avoid using "
#         "all capital letters. In comics, it is common to use two hyphens '--' to interrupt a sentence. "
#         "Retain any hyphens as they appear in the original text. Provide the transcribed text "
#         "clearly, ensuring it is capitalized where appropriate, including proper nouns.")

prompt_text_tmpl = (
        "Please perform optical character recognition (OCR) on this image, which displays "
        "speech balloons from a comic book. The text is in {}. Extract the text and "
        "format it as follows: transcribe in standard sentence case, capitalized. Avoid using "
        "all capital letters, but ensure it is capitalized where appropriate, including proper nouns. "
        "Provide the transcribed text clearly. Double check the text is not all capital letters.")


# prompt_text_tmpl = (
#         "Please perform optical character recognition (OCR) on this image, which contains speech "
#         "balloons from a comic book. The text is in English. Carefully transcribe the text, "
#         "ensuring that you preserve the original formatting and line breaks as they appear "
#         "in the speech balloon."
# )

default_prompt_text_tmpl = prompt_text_tmpl

# %% ../nbs/ocr_idefics.ipynb 48
class IdeficsOCR:
    prompt_text_tmpl: str = default_prompt_text_tmpl
    PROCESSOR: Any = None
    MODEL: Any = None


    @classmethod
    def setup_processor(cls):
        cls.PROCESSOR = _setup_processor()
        return cls.PROCESSOR
    
    @classmethod
    def setup_model(cls, quant: QuantT='bfloat16', flashattn: bool=True):
        cls.MODEL = _setup_model(quant, flashattn)
        return cls.MODEL
    
    @staticmethod
    def is_idefics_available() -> bool:
        return True

    def show_info(self):
        cprint(
            f"{'model':>17}: {type(self.MODEL)}\n"
            f"{'quantization':>17}: {type(self.quant)}\n"
            f"{'device':>17}: {repr(self.MODEL.device)}\n"
            f"{'current VRAM':>17}: {get_gpu_vram(False)}  MiB\n"
    )


    def __init__(self, 
            lang: str | None = None, 
            prompt_text_tmpl: str|None = None, 
            quant: QuantT | None = None,
            flashattn: bool | None = None,
        ):
        self.lang = lang
        self.prompt_text_tmpl = prompt_text_tmpl or self.prompt_text_tmpl
        self.quant = quant or 'bfloat16'#'4bits'
        self.flashattn = flashattn or True
        if self.PROCESSOR is None:
            type(self).setup_processor()
        if self.MODEL is None:
            type(self).setup_model(self.quant, self.flashattn)
        self.device = self.MODEL.device

    def _generation_args(self, image: Image.Image, resulting_messages: list[dict]):
        prompt = self.PROCESSOR.apply_chat_template(resulting_messages, add_generation_prompt=True)
        inputs = self.PROCESSOR(text=prompt, images=[image], return_tensors="pt")
        inputs = {k: v.to(self.device) for k, v in inputs.items()}
        
        max_new_tokens = 512
        repetition_penalty = 1.2
        decoding_strategy = "Greedy"
        temperature = 0.4
        top_p = 0.8

        generation_args = {
            "max_new_tokens": max_new_tokens,
            "repetition_penalty": repetition_penalty,
        }

        assert decoding_strategy in [
            "Greedy",
            "Top P Sampling",
        ]

        if decoding_strategy == "Greedy":
            generation_args["do_sample"] = False
        elif decoding_strategy == "Top P Sampling":
            generation_args["temperature"] = temperature
            generation_args["do_sample"] = True
            generation_args["top_p"] = top_p

        generation_args.update(inputs)
        return prompt, generation_args

    def __call__(
        self,
        img_or_path: Image.Image | Path | str,
        prompt_text: str | None = None,
        lang: str | None = None,
        config: str | None = None,
        show_prompt: bool = False,
        **kwargs,
    ) -> str:
        if not self.is_idefics_available():
            raise RuntimeError("Idefics is not installed or not found.")
        resulting_messages = [
            {
                "role": "user",
                "content": [{"type": "image"}] + [
                    {"type": "text", "text": prompt_text or self.prompt_text_tmpl.format(lang or self.lang)}
                ]
            }
        ]
        image = load_image(img_or_path)
        prompt, generation_args = self._generation_args(image, resulting_messages)
        generated_ids = self.MODEL.generate(**generation_args)
        generated_texts = self.PROCESSOR.batch_decode(
            generated_ids[:, generation_args["input_ids"].size(1):], skip_special_tokens=True)
        if show_prompt:
            cprint("INPUT:", prompt, "|OUTPUT:", generated_texts)
        return generated_texts[0]#.strip('"')

    def postprocess_ocr(self, text):
        return ' '.join(remove_multiple_whitespaces(text).splitlines())


# %% ../nbs/ocr_idefics.ipynb 50
class IdeficsExperimentContext(OCRExperimentContext):
    @functools.lru_cache()
    def mocr(self, lang: str):
        if self.ocr_model == 'Idefics':
            proc = IdeficsOCR(lang)
        else:
            engine = self.engines[self.ocr_model]
            ocr_processor = ocr.get_ocr_processor(True, engine)
            proc = ocr_processor[lang2pcleaner(lang)]
            if isinstance(proc, TesseractOcr):
                proc.lang = lang2tesseract(lang)
        return proc

    def cleanup_model(self):
        del IdeficsOCR.MODEL
        torch.cuda.empty_cache()
        import gc
        gc.collect()
        IdeficsOCR.MODEL = None

    def setup_idefics(self, quant: QuantT = 'bfloat16', flashattn: bool = True):
        if IdeficsOCR.PROCESSOR is None:
            IdeficsOCR.setup_processor()
        if IdeficsOCR.MODEL is not None:
            self.cleanup_model()
        if IdeficsOCR.MODEL is None:
            IdeficsOCR.setup_model(quant=quant, flashattn=flashattn)

    def show(self):
        super().show()
        cfg = IdeficsOCR.MODEL.config
        if hasattr(cfg, 'quantization_config'):
            qcfg = cfg.quantization_config
            quant = '4bits' if qcfg.load_in_4bit else '8bits'
        else:
            quant = 'bfloat16'
        cprint(
            f"{'Quantization':>17}: {quant!r}\n"
            f"{'Flash attention 2':>17}: {cfg._attn_implementation == 'flash_attention_2'}\n"
            f"{'VRAM':>17}: {get_gpu_vram(False)}/{get_gpu_vram()} MiB\n"
        )

    def __init__(self, 
            root_dir: Path | str | None = None, 
            quant: QuantT = 'bfloat16', 
            flashattn: bool = True,
            *, 
            config: cfg.Config | None = None, 
            server: web_server.WebServer | None = None,
            run_name: str = 'Idefics-crop-post', 
            setup_idefics: bool = True,
        ):
        super().__init__('Idefics', root_dir, config=config, server=server, run_name=run_name)
        if setup_idefics:
            self.setup_idefics(quant, flashattn)


